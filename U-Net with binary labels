{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1900145,"sourceType":"datasetVersion","datasetId":693124}],"dockerImageVersionId":29956,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"U-Net with binary labels","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/u-net-with-binary-labels-deba3241-f9a5-4a09-b12b-ac1405efec08.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240428/auto/storage/goog4_request&X-Goog-Date=20240428T174905Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2b098719bec5f7fb8999ed860865b523018c65d41f8384df074a729251e7c8b16c4a99c6098ffc45c2ea43645dd1bcf2e34b9f274eb7c7930d3d731e5f232bf55f3c4a2354ab0e4d1f5e68fe76e8ea39a60fc14ae8f05b21608c32cdffce1d29b412874a11fe663a3a82e2d8aa8722ae4c1f91e230303c2327056bb4706e0653249497226559c58d9606c4b2dc9144ca7d5ad7c207f187649e833ced60f707dcd1a8d18ccb0c15fbb77bf62d5fbd92e4363cdd1fd13acfd3469b58470f5d39ff5be7ba1572c10674837ba181f3870fa4f407239adec6b296166e95366659956c5f291ba8ecd8770f98220b7e69d3dccc2f25e79a22644eace5df6d78006d9316","timestamp":1714326978153}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'segmentation-of-nuclei-in-cryosectioned-he-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F693124%2F1900145%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240428%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240428T174905Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D76a6412838b4e8a714ed0f1c3b1f5e6e708b11b82ba34de3c27b0c55f5b36dee75d5c4fcb3f27691da2683dde8e906569200e1cd08cca70293fd9e0a81264c252eecf9ec1e7cf63dd726a9406f77ca59e252f59cf627a27a2fb40e2170b5280221aa12f7072c066b2ff6d5ff5b2e19f3a75a130abc96f447d75e1345d2d64f0c9847215cb2ae4113a1fa83b94b1897b2927ed6aeb55e6c111338fc0b8537b0ea9cacc81e8cb7967e47cc44e8968fb5d08d5540d60cdce9a0c6e8eca3c8c1fa7c29b83d98de5959c0ea7d22d47b4ae357ed3c113afca9b14360cd24e9d1fc18289a53f42da02b21555fad06d5525970cc6235afadc538a71f06707d4185d94309'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"ZIW7Iyyd0y1X"},"cell_type":"code","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# U-Net standard model (with binary labels)"],"metadata":{"id":"0Om85W5t0y1Y"}},{"cell_type":"markdown","source":["Here you set all parameters thay you may need for training and testing"],"metadata":{"id":"P3MSFmh40y1Z"}},{"cell_type":"code","source":["opts = {}\n","#opts['tf_version'] = 1.14                      # current version also works with tf 2.2\n","opts['imageType_train'] = '.tif'\n","opts['imageType_test'] = '.tif'\n","opts['number_of_channel'] = 3                   # Set if to '3' for RGB images and set it to '1' for grayscale images\n","opts['treshold'] = 0.5                          # treshold to convert the network output (stage 1) to binary masks\n","## input & output directories\n","opts['train_dir'] = '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/'\n","opts['train_label_dir'] = '../input/segmentation-of-nuclei-in-cryosectioned-he-images/Annotator 1 (biologist)/mask binary/'\n","opts['train_label_masks'] = '../input/segmentation-of-nuclei-in-cryosectioned-he-images/Annotator 1 (biologist)/label masks modify/'\n","opts['train_dis_dir'] = '../input/segmentation-of-nuclei-in-cryosectioned-he-images/Annotator 1 (biologist)/distance maps/'\n","opts['results_save_path'] ='/kaggle/working/images/'\n","opts['models_save_path'] ='/kaggle/working/models/'\n","\n","opts['epoch_num_stage1'] = 20                   # number of epochs for stage 1\n","opts['quick_run'] = 0.01                         # step = (len(train)/batch_size) / quick_run (set it to large numbers just debugging the code)\n","opts['batch_size'] = 16                          # batch size\n","opts['random_seed_num'] = 19                    # keep it constant to be able to reproduce the results\n","opts['k_fold'] = 10                             # set to '1' to have no cross validation (much faster training but 2-3% degradation in performance)\n","opts['save_val_results'] = 1                    # set to '0' to skip saving the validation results in training\n","opts['init_LR'] = 0.001                         # initial learning rate for stage 1 and stage 2\n","opts['LR_decay_factor'] = 0.5                   # learning rate scheduler\n","opts['LR_drop_after_nth_epoch'] = 8            # learning rate scheduler\n","opts['crop_size'] = 512                         # crop size for training\n","opts['pretrained_model'] = 'efficientnetb0'     # future development\n","opts['use_pretrained_flag'] = 0                 # if you want to use a pretrained model in the encoder set it to one\n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-28T15:21:53.47422Z","iopub.execute_input":"2024-04-28T15:21:53.474549Z","iopub.status.idle":"2024-04-28T15:21:53.485765Z","shell.execute_reply.started":"2024-04-28T15:21:53.474521Z","shell.execute_reply":"2024-04-28T15:21:53.484764Z"},"trusted":true,"id":"_49fGY8e0y1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## disabeling warning msg\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","# 0 = all messages are logged (default behavior)\n","# 1 = INFO messages are not printed\n","# 2 = INFO and WARNING messages are not printed\n","# 3 = INFO, WARNING, and ERROR messages are not printed\n","import warnings\n","warnings.simplefilter('ignore')\n","import sys\n","sys.stdout.flush() # resolving tqdm problem"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:53.487324Z","iopub.execute_input":"2024-04-28T15:21:53.487634Z","iopub.status.idle":"2024-04-28T15:21:53.501332Z","shell.execute_reply.started":"2024-04-28T15:21:53.4876Z","shell.execute_reply":"2024-04-28T15:21:53.50061Z"},"trusted":true,"id":"WouY63Tv0y1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["importing required libraries"],"metadata":{"id":"8_ylvwD60y1a"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import math\n","from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n","import random\n","\n","import keras\n","from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, add\n","from keras.layers.core import Dropout, Lambda\n","from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers.merge import concatenate\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","from keras.models import Model, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler,CSVLogger\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from keras.optimizers import Adam\n","from keras.callbacks import LearningRateScheduler\n","from keras.losses import categorical_crossentropy\n","from keras.optimizers import Adam\n","#import segmentation_models as sm\n","from albumentations import*\n","import cv2\n","from random import shuffle                            #\n","import os\n","import matplotlib.pyplot as plt\n","from skimage.io import imsave\n","\n","\n","import time                                           # measuring training and test time\n","from glob import glob                                 # path control\n","import tqdm\n","from scipy.ndimage.morphology import binary_fill_holes\n","from skimage.morphology import remove_small_objects\n","from scipy.ndimage.filters import gaussian_filter\n","import skimage.morphology\n","from skimage import io, exposure, img_as_uint, img_as_float\n","from skimage.io import imsave, imread\n","from skimage.morphology import label\n","from skimage.morphology import watershed\n","from skimage.feature import peak_local_max\n","#import segmentation_models as sm\n","from scipy import ndimage as ndi"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:53.502519Z","iopub.execute_input":"2024-04-28T15:21:53.502837Z","iopub.status.idle":"2024-04-28T15:21:59.828809Z","shell.execute_reply.started":"2024-04-28T15:21:53.502776Z","shell.execute_reply":"2024-04-28T15:21:59.827868Z"},"trusted":true,"id":"BYWaHFl10y1a","outputId":"416d2757-dda2-4bed-ca4c-eaed7a817b98"},"execution_count":null,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:59.830136Z","iopub.execute_input":"2024-04-28T15:21:59.830506Z","iopub.status.idle":"2024-04-28T15:21:59.837144Z","shell.execute_reply.started":"2024-04-28T15:21:59.830467Z","shell.execute_reply":"2024-04-28T15:21:59.836196Z"},"trusted":true,"id":"t2r4yN5E0y1a","outputId":"f50cc6ed-2405-4d5c-dca1-074c6961a083"},"execution_count":null,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'2.2.0'"},"metadata":{}}]},{"cell_type":"markdown","source":["defining functions that are used in training and testing"],"metadata":{"id":"J3OOgz660y1b"}},{"cell_type":"code","source":["# Dice loss function\n","def dice_coef(y_true, y_pred):\n","    smooth = 1.\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1 - dice_coef(y_true, y_pred)\n","#####################################################################################\n","# Combination of Dice and binary cross entophy loss function\n","def bce_dice_loss(y_true, y_pred):\n","    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n","########################################################################################\n","# custom callsback (decaying learning rate)\n","def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, epochs_drop=1000):\n","    '''\n","    Wrapper function to create a LearningRateScheduler with step decay schedule.\n","    '''\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/epochs_drop))\n","\n","    return LearningRateScheduler(schedule, verbose = 1)\n","#######################################################################################################\n","def binary_unet( IMG_CHANNELS, LearnRate):\n","    inputs = Input((None, None, IMG_CHANNELS))\n","    #s = Lambda(lambda x: x / 255) (inputs)\n","\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (inputs)\n","    c1 = Dropout(0.1) (c1)\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p1)\n","    c2 = Dropout(0.1) (c2)\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p2)\n","    c3 = Dropout(0.1) (c3)\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p3)\n","    c4 = Dropout(0.1) (c4)\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (p4)\n","    c5 = Dropout(0.1) (c5)\n","    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c5)\n","\n","    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n","    u6 = concatenate([u6, c4])\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u6)\n","    c6 = Dropout(0.1) (c6)\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n","\n","    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n","    u7 = concatenate([u7, c3])\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u7)\n","    c7 = Dropout(0.1) (c7)\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n","\n","    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n","    u8 = concatenate([u8, c2])\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u8)\n","    c8 = Dropout(0.1) (c8)\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n","\n","    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n","    u9 = concatenate([u9, c1], axis=3)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (u9)\n","    c9 = Dropout(0.1) (c9)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n","\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9) # for binary\n","\n","    model = Model(inputs=[inputs], outputs=[outputs])\n","    model.compile(optimizer = Adam(lr=LearnRate), loss= bce_dice_loss , metrics=[dice_coef]) #for binary\n","\n","    #model.summary()\n","    return model\n","#######################################################################################################\n","def deeper_binary_unet(IMG_CHANNELS, LearnRate):\n","    # Build U-Net model\n","    inputs = Input((None, None, IMG_CHANNELS))\n","    #s = Lambda(lambda x: x / 255) (inputs)\n","\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n","    c1 = Dropout(0.1) (c1)\n","    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n","    c2 = Dropout(0.1) (c2)\n","    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n","    c3 = Dropout(0.1) (c3)\n","    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n","    c4 = Dropout(0.1) (c4)\n","    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","\n","    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n","    c4_new = Dropout(0.1) (c4_new)\n","    c4_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4_new)\n","    p4_new = MaxPooling2D(pool_size=(2, 2)) (c4_new)\n","\n","    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4_new)\n","    c5 = Dropout(0.1) (c5)\n","    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n","\n","\n","    u6_new = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n","    u6_new = concatenate([u6_new, c4_new])\n","    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6_new)\n","    c6_new = Dropout(0.1) (c6_new)\n","    c6_new = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6_new)\n","\n","    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6_new)\n","    u6 = concatenate([u6, c4])\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n","    c6 = Dropout(0.1) (c6)\n","    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n","\n","    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n","    u7 = concatenate([u7, c3])\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n","    c7 = Dropout(0.1) (c7)\n","    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n","\n","    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n","    u8 = concatenate([u8, c2])\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n","    c8 = Dropout(0.1) (c8)\n","    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n","\n","    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n","    u9 = concatenate([u9, c1], axis=3)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n","    c9 = Dropout(0.1) (c9)\n","    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n","\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n","\n","    model_deeper = Model(inputs=[inputs], outputs=[outputs])\n","    model_deeper.compile(optimizer = Adam(lr=LearnRate), loss= bce_dice_loss , metrics=[ dice_coef])\n","    #model_deeper.summary()\n","    return model_deeper"],"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-04-28T15:21:59.840113Z","iopub.execute_input":"2024-04-28T15:21:59.840415Z","iopub.status.idle":"2024-04-28T15:21:59.913141Z","shell.execute_reply.started":"2024-04-28T15:21:59.840388Z","shell.execute_reply":"2024-04-28T15:21:59.912311Z"},"trusted":true,"id":"anUuFWrd0y1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# augmentation function\n","def albumentation_aug(p=1.0, crop_size_row = 448, crop_size_col = 448 ):\n","    return Compose([\n","        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1),\n","        CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n","        RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, brightness_by_max=True, p=0.4),\n","        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.1),\n","        HorizontalFlip(always_apply=False, p=0.5),\n","        VerticalFlip(always_apply=False, p=0.5),\n","        RandomRotate90(always_apply=False, p=0.5),\n","        #ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.3),\n","    ], p=p) # --> this p has the second proiroty comapred to the p inside each argument (e.g. HorizontalFlip(always_apply=False, p=0.5) )\n","###########################################################\n","def albumentation_aug_light(p=1.0, crop_size_row = 448, crop_size_col = 448):\n","    return Compose([\n","        RandomCrop(crop_size_row, crop_size_col, always_apply=True, p=1.0),\n","        HorizontalFlip(always_apply=False, p=0.5),\n","        VerticalFlip(always_apply=False, p=0.5),\n","        RandomRotate90(always_apply=False, p=0.5),\n","        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.1),\n","    ], p=p, additional_targets={'mask1': 'mask','mask2': 'mask'}) # --> this p has the second proiroty comapred to the p inside each argument (e.g. HorizontalFlip(always_apply=False, p=0.5) )\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:59.915376Z","iopub.execute_input":"2024-04-28T15:21:59.915766Z","iopub.status.idle":"2024-04-28T15:21:59.928745Z","shell.execute_reply.started":"2024-04-28T15:21:59.915727Z","shell.execute_reply":"2024-04-28T15:21:59.928031Z"},"trusted":true,"id":"Bi1_Chox0y1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* evaluation indexes (from the hovernet paper: https://github.com/vqdang/hover_net/blob/master/src/metrics/stats_utils.py)"],"metadata":{"id":"5V_tLPey0y1c"}},{"cell_type":"code","source":["def get_dice_1(true, pred):\n","    \"\"\"\n","        Traditional dice\n","    \"\"\"\n","    # cast to binary 1st\n","    true = np.copy(true)\n","    pred = np.copy(pred)\n","    true[true > 0] = 1\n","    pred[pred > 0] = 1\n","    inter = true * pred\n","    denom = true + pred\n","    return 2.0 * np.sum(inter) / np.sum(denom)\n","##############################################################################################\n","def get_fast_aji(true, pred):\n","    \"\"\"\n","    AJI version distributed by MoNuSeg, has no permutation problem but suffered from\n","    over-penalisation similar to DICE2\n","    Fast computation requires instance IDs are in contiguous orderding i.e [1, 2, 3, 4]\n","    not [2, 3, 6, 10]. Please call `remap_label` before hand and `by_size` flag has no\n","    effect on the result.\n","    \"\"\"\n","    true = np.copy(true) # ? do we need this\n","    pred = np.copy(pred)\n","    true_id_list = list(np.unique(true))\n","    pred_id_list = list(np.unique(pred))\n","\n","    true_masks = [None,]\n","    for t in true_id_list[1:]:\n","        t_mask = np.array(true == t, np.uint8)\n","        true_masks.append(t_mask)\n","\n","    pred_masks = [None,]\n","    for p in pred_id_list[1:]:\n","        p_mask = np.array(pred == p, np.uint8)\n","        pred_masks.append(p_mask)\n","\n","    # prefill with value\n","    pairwise_inter = np.zeros([len(true_id_list) -1,\n","                               len(pred_id_list) -1], dtype=np.float64)\n","    pairwise_union = np.zeros([len(true_id_list) -1,\n","                               len(pred_id_list) -1], dtype=np.float64)\n","\n","    # caching pairwise\n","    for true_id in true_id_list[1:]: # 0-th is background\n","        t_mask = true_masks[true_id]\n","        pred_true_overlap = pred[t_mask > 0]\n","        pred_true_overlap_id = np.unique(pred_true_overlap)\n","        pred_true_overlap_id = list(pred_true_overlap_id)\n","        for pred_id in pred_true_overlap_id:\n","            if pred_id == 0: # ignore\n","                continue # overlaping background\n","            p_mask = pred_masks[pred_id]\n","            total = (t_mask + p_mask).sum()\n","            inter = (t_mask * p_mask).sum()\n","            pairwise_inter[true_id-1, pred_id-1] = inter\n","            pairwise_union[true_id-1, pred_id-1] = total - inter\n","    #\n","    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n","    # pair of pred that give highest iou for each true, dont care\n","    # about reusing pred instance multiple times\n","    paired_pred = np.argmax(pairwise_iou, axis=1)\n","    pairwise_iou = np.max(pairwise_iou, axis=1)\n","    # exlude those dont have intersection\n","    paired_true = np.nonzero(pairwise_iou > 0.0)[0]\n","    paired_pred = paired_pred[paired_true]\n","    # print(paired_true.shape, paired_pred.shape)\n","    overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n","    overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n","    #\n","    paired_true = (list(paired_true + 1)) # index to instance ID\n","    paired_pred = (list(paired_pred + 1))\n","    # add all unpaired GT and Prediction into the union\n","    unpaired_true = np.array([idx for idx in true_id_list[1:] if idx not in paired_true])\n","    unpaired_pred = np.array([idx for idx in pred_id_list[1:] if idx not in paired_pred])\n","    for true_id in unpaired_true:\n","        overall_union += true_masks[true_id].sum()\n","    for pred_id in unpaired_pred:\n","        overall_union += pred_masks[pred_id].sum()\n","    #\n","    aji_score = overall_inter / overall_union\n","    return aji_score\n","##############################################################################################\n","def remap_label(pred, by_size=False):\n","    \"\"\"\n","    Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3]\n","    not [0, 2, 4, 6]. The ordering of instances (which one comes first)\n","    is preserved unless by_size=True, then the instances will be reordered\n","    so that bigger nucler has smaller ID\n","    Args:\n","        pred    : the 2d array contain instances where each instances is marked\n","                  by non-zero integer\n","        by_size : renaming with larger nuclei has smaller id (on-top)\n","    \"\"\"\n","    pred_id = list(np.unique(pred))\n","    pred_id.remove(0)\n","    if len(pred_id) == 0:\n","        return pred # no label\n","    if by_size:\n","        pred_size = []\n","        for inst_id in pred_id:\n","            size = (pred == inst_id).sum()\n","            pred_size.append(size)\n","        # sort the id by size in descending order\n","        pair_list = zip(pred_id, pred_size)\n","        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n","        pred_id, pred_size = zip(*pair_list)\n","\n","    new_pred = np.zeros(pred.shape, np.int32)\n","    for idx, inst_id in enumerate(pred_id):\n","        new_pred[pred == inst_id] = idx + 1\n","    return new_pred\n","\n","##############################################################################################\n","def get_fast_pq(true, pred, match_iou=0.5):\n","    \"\"\"\n","    `match_iou` is the IoU threshold level to determine the pairing between\n","    GT instances `p` and prediction instances `g`. `p` and `g` is a pair\n","    if IoU > `match_iou`. However, pair of `p` and `g` must be unique\n","    (1 prediction instance to 1 GT instance mapping).\n","    If `match_iou` < 0.5, Munkres assignment (solving minimum weight matching\n","    in bipartite graphs) is caculated to find the maximal amount of unique pairing.\n","    If `match_iou` >= 0.5, all IoU(p,g) > 0.5 pairing is proven to be unique and\n","    the number of pairs is also maximal.\n","    Fast computation requires instance IDs are in contiguous orderding\n","    i.e [1, 2, 3, 4] not [2, 3, 6, 10]. Please call `remap_label` beforehand\n","    and `by_size` flag has no effect on the result.\n","    Returns:\n","        [dq, sq, pq]: measurement statistic\n","        [paired_true, paired_pred, unpaired_true, unpaired_pred]:\n","                      pairing information to perform measurement\n","    \"\"\"\n","    assert match_iou >= 0.0, \"Cant' be negative\"\n","\n","    true = np.copy(true)\n","    pred = np.copy(pred)\n","    true_id_list = list(np.unique(true))\n","    pred_id_list = list(np.unique(pred))\n","\n","    true_masks = [None, ]\n","    for t in true_id_list[1:]:\n","        t_mask = np.array(true == t, np.uint8)\n","        true_masks.append(t_mask)\n","\n","    pred_masks = [None, ]\n","    for p in pred_id_list[1:]:\n","        p_mask = np.array(pred == p, np.uint8)\n","        pred_masks.append(p_mask)\n","\n","    # prefill with value\n","    pairwise_iou = np.zeros([len(true_id_list) - 1,\n","                             len(pred_id_list) - 1], dtype=np.float64)\n","\n","    # caching pairwise iou\n","    for true_id in true_id_list[1:]:  # 0-th is background\n","        t_mask = true_masks[true_id]\n","        pred_true_overlap = pred[t_mask > 0]\n","        pred_true_overlap_id = np.unique(pred_true_overlap)\n","        pred_true_overlap_id = list(pred_true_overlap_id)\n","        for pred_id in pred_true_overlap_id:\n","            if pred_id == 0:  # ignore\n","                continue  # overlaping background\n","            p_mask = pred_masks[pred_id]\n","            total = (t_mask + p_mask).sum()\n","            inter = (t_mask * p_mask).sum()\n","            iou = inter / (total - inter)\n","            pairwise_iou[true_id - 1, pred_id - 1] = iou\n","    #\n","    if match_iou >= 0.5:\n","        paired_iou = pairwise_iou[pairwise_iou > match_iou]\n","        pairwise_iou[pairwise_iou <= match_iou] = 0.0\n","        paired_true, paired_pred = np.nonzero(pairwise_iou)\n","        paired_iou = pairwise_iou[paired_true, paired_pred]\n","        paired_true += 1  # index is instance id - 1\n","        paired_pred += 1  # hence return back to original\n","    else:  # * Exhaustive maximal unique pairing\n","        #### Munkres pairing with scipy library\n","        # the algorithm return (row indices, matched column indices)\n","        # if there is multiple same cost in a row, index of first occurence\n","        # is return, thus the unique pairing is ensure\n","        # inverse pair to get high IoU as minimum\n","        paired_true, paired_pred = linear_sum_assignment(-pairwise_iou)\n","        ### extract the paired cost and remove invalid pair\n","        paired_iou = pairwise_iou[paired_true, paired_pred]\n","\n","        # now select those above threshold level\n","        # paired with iou = 0.0 i.e no intersection => FP or FN\n","        paired_true = list(paired_true[paired_iou > match_iou] + 1)\n","        paired_pred = list(paired_pred[paired_iou > match_iou] + 1)\n","        paired_iou = paired_iou[paired_iou > match_iou]\n","\n","    # get the actual FP and FN\n","    unpaired_true = [idx for idx in true_id_list[1:] if idx not in paired_true]\n","    unpaired_pred = [idx for idx in pred_id_list[1:] if idx not in paired_pred]\n","    # print(paired_iou.shape, paired_true.shape, len(unpaired_true), len(unpaired_pred))\n","\n","    #\n","    tp = len(paired_true)\n","    fp = len(unpaired_pred)\n","    fn = len(unpaired_true)\n","    # get the F1-score i.e DQ\n","    dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n","    # get the SQ, no paired has 0 iou so not impact\n","    sq = paired_iou.sum() / (tp + 1.0e-6)\n","\n","    return [dq, sq, dq * sq], [paired_true, paired_pred, unpaired_true, unpaired_pred]"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:59.93001Z","iopub.execute_input":"2024-04-28T15:21:59.930286Z","iopub.status.idle":"2024-04-28T15:21:59.978572Z","shell.execute_reply.started":"2024-04-28T15:21:59.930259Z","shell.execute_reply":"2024-04-28T15:21:59.977977Z"},"trusted":true,"id":"WToixjaQ0y1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# other useful finction for training\n","\n","def get_id_from_file_path(file_path, indicator):\n","    return file_path.split(os.path.sep)[-1].replace(indicator, '')\n","############################################################\n","def chunker(seq, seq2, size):\n","    return ([seq[pos:pos + size], seq2[pos:pos + size]] for pos in range(0, len(seq), size))\n","############################################################\n","def data_gen_heavy(list_files, list_files2, batch_size, p , size_row, size_col, distance_unet_flag = 0, augment=False, BACKBONE_model = 'efficientnetb0', use_pretrain_flag =1):\n","    #preprocess_input = sm.get_preprocessing(BACKBONE_model)\n","    crop_size_row = size_row\n","    crop_size_col = size_col\n","    aug = albumentation_aug(p, crop_size_row, crop_size_col)\n","\n","    while True:\n","        #shuffle(list_files)\n","        for batch in chunker(list_files,list_files2, batch_size):\n","            #X = [cv2.resize(cv2.imread(x), (size, size)) for x in batch]\n","            X = []\n","            Y = []\n","\n","            for count in range(len(batch[0])):\n","                # x = cv2.resize(cv2.imread(batch[0][count]), (size_col, size_row))\n","                # x_mask = cv2.resize(cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE), (size_col, size_row))\n","                x = cv2.imread(batch[0][count])\n","                x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","                x_mask = cv2.imread(batch[1][count], cv2.IMREAD_GRAYSCALE)\n","\n","                x_mask_temp = np.zeros((x_mask.shape[0], x_mask.shape[1]))\n","                x_mask_temp[x_mask == 255] = 1\n","\n","\n","                if distance_unet_flag == False:\n","                    if augment:\n","                        augmented = aug(image= x, mask= x_mask_temp)\n","                        x = augmented['image']\n","                        if use_pretrain_flag == 1:\n","                            x = preprocess_input(x)\n","                        x_mask_temp = augmented['mask']\n","                        x = x/255\n","                    X.append(x)\n","                    Y.append(x_mask_temp)\n","                    #imsave('/media/masih/wd/projects/MoNuSAC_binary/results/images/an/{}_binary.png'.format(get_id_from_file_path(batch[0][count], '.png')), x_mask_epithelial)\n","                    #imsave('/media/masih/wd/projects/MoNuSAC_binary/results/images/an/{}.png'.format(get_id_from_file_path(batch[0][count], '.tif')), x)\n","                else:\n","                    if augment:\n","                        augmented = aug(image=x, mask=x_mask)\n","                        x = augmented['image']\n","                        if use_pretrain_flag == 1:\n","                            x = preprocess_input(x)\n","                        x_mask = augmented['mask']\n","\n","                    X.append(x)\n","                    x_mask = (x_mask - np.min(x_mask))/ (np.max(x_mask) - np.min(x_mask) + 0.0000001)\n","                    Y.append(x_mask)\n","\n","                del x_mask\n","                del x_mask_temp\n","                del x\n","            Y = np.expand_dims(np.array(Y), axis=3)\n","            Y = np.array(Y)\n","            yield np.array(X), np.array(Y)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:21:59.979758Z","iopub.execute_input":"2024-04-28T15:21:59.980057Z","iopub.status.idle":"2024-04-28T15:22:00.000612Z","shell.execute_reply.started":"2024-04-28T15:21:59.98003Z","shell.execute_reply":"2024-04-28T15:21:59.999758Z"},"trusted":true,"id":"cvfauaak0y1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create folders to save the best models and images (if needed) for each fold\n","if not os.path.exists('/kaggle/working/images/'):\n","    os.makedirs('/kaggle/working/images/')\n","if not os.path.exists('/kaggle/working/models/'):\n","    os.makedirs('/kaggle/working/models/')\n","if not os.path.exists(opts['results_save_path']+ 'stage1/validation/pure_unet'):\n","    os.makedirs(opts['results_save_path'] + 'stage1/validation/pure_unet')\n","if not os.path.exists(opts['results_save_path']+ 'stage1/validation/watershed_unet'):\n","    os.makedirs(opts['results_save_path'] + 'stage1/validation/watershed_unet')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:22:00.001616Z","iopub.execute_input":"2024-04-28T15:22:00.001973Z","iopub.status.idle":"2024-04-28T15:22:00.01669Z","shell.execute_reply.started":"2024-04-28T15:22:00.001938Z","shell.execute_reply":"2024-04-28T15:22:00.015953Z"},"trusted":true,"id":"8WjfUNGM0y1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_files = glob('{}*{}'.format(opts['train_dir'], opts['imageType_train']))\n","train_files_mask = glob('{}*.png'.format(opts['train_label_dir']))\n","train_files_dis = glob('{}*.png'.format(opts['train_dis_dir']))\n","train_files_labels = glob('{}*.tif'.format(opts['train_label_masks']))\n","\n","\n","train_files.sort()\n","train_files_mask.sort()\n","train_files_dis.sort()\n","train_files_labels.sort()\n","print(\"Total number of training images:\", len(train_files))"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:22:00.017672Z","iopub.execute_input":"2024-04-28T15:22:00.017996Z","iopub.status.idle":"2024-04-28T15:22:00.074116Z","shell.execute_reply.started":"2024-04-28T15:22:00.01797Z","shell.execute_reply":"2024-04-28T15:22:00.073314Z"},"trusted":true,"id":"A9CeGqrc0y1d","outputId":"530a1983-97cf-4a7d-ab85-c38aae28fc6e"},"execution_count":null,"outputs":[{"name":"stdout","text":"Total number of training images: 30\n","output_type":"stream"}]},{"cell_type":"code","source":["# we have 10 organ in this dataset\n","train_files"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:22:00.075053Z","iopub.execute_input":"2024-04-28T15:22:00.075295Z","iopub.status.idle":"2024-04-28T15:22:00.080685Z","shell.execute_reply.started":"2024-04-28T15:22:00.07527Z","shell.execute_reply":"2024-04-28T15:22:00.079874Z"},"trusted":true,"id":"4WOxoP2y0y1d","outputId":"41402c17-4408-4fc5-f335-5ba727bb75ef"},"execution_count":null,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_AdrenalGland_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_AdrenalGland_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_AdrenalGland_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Larynx_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Larynx_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Larynx_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_LymphNodes_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_LymphNodes_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_LymphNodes_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Mediastinum_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Mediastinum_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Mediastinum_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pancreas_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pancreas_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pancreas_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pleura_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pleura_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Pleura_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Skin_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Skin_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Skin_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Testes_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Testes_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Testes_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Thymus_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Thymus_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_Thymus_03.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_ThyroidGland_01.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_ThyroidGland_02.tif',\n '../input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_ThyroidGland_03.tif']"},"metadata":{}}]},{"cell_type":"code","source":["# creating 10 folds to perfrom 10 fold cross-validation (for each fold images from the 9 organs are used for training and the images from one organ are used as validation)\n","\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold1 = train_files[0: int(np.round(len(train_files) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold\" + str(k + 1)] = train_files[int(np.round(len (train_files) / opts['k_fold']) * k): int(np.round(len(train_files) / opts['k_fold']) * (k+1))]\n","print(\"length of each fold:\", len(fold1))\n","\n","# for binary mask\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_mask1 = train_files_mask[0: int(np.round(len(train_files_mask) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_mask\" + str(k + 1)] = train_files_mask[int(np.round(len (train_files_mask) / opts['k_fold']) * k): int(np.round(len(train_files_mask) / opts['k_fold']) * (k+1))]\n","\n","# for distance mask\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_dis1 = train_files_dis[0: int(np.round(len(train_files_dis) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_dis\" + str(k + 1)] = train_files_dis[int(np.round(len (train_files_dis) / opts['k_fold']) * k): int(np.round(len(train_files_dis) / opts['k_fold']) * (k+1))]\n","\n","# for label masks (just for evaluation)\n","for k in range(opts['k_fold']):\n","    if k ==0:\n","        fold_label1 = train_files_labels[0: int(np.round(len(train_files_labels) / opts['k_fold']))]\n","    else:\n","        globals()[\"fold_label\" + str(k + 1)] = train_files_labels[int(np.round(len (train_files_labels) / opts['k_fold']) * k): int(np.round(len(train_files_labels) / opts['k_fold']) * (k+1))]\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:22:00.082284Z","iopub.execute_input":"2024-04-28T15:22:00.082698Z","iopub.status.idle":"2024-04-28T15:22:00.102306Z","shell.execute_reply.started":"2024-04-28T15:22:00.082661Z","shell.execute_reply":"2024-04-28T15:22:00.101313Z"},"trusted":true,"id":"yDY4Qh-Y0y1d","outputId":"99ffb986-12b2-409f-f176-d6f82a407578"},"execution_count":null,"outputs":[{"name":"stdout","text":"length of each fold: 3\n","output_type":"stream"}]},{"cell_type":"code","source":["# main training loop (for all 10 fold cross-validation)\n","start_time = time.time()\n","dice_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","AJI_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","PQ_pure_unet = np.zeros([opts['k_fold'],len(fold1)])\n","\n","dice_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","AJI_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","PQ_unet_watershed = np.zeros([opts['k_fold'],len(fold1)])\n","\n","\n","\n","for K_fold in range(opts['k_fold']):\n","    train = []\n","    train_mask = []\n","    train_dis = []\n","\n","    val = eval('fold' + str(K_fold + 1))\n","    val_mask = eval('fold_mask' + str(K_fold + 1))\n","    val_dis = eval('fold_dis' + str(K_fold + 1))\n","    val_label = eval('fold_label' + str(K_fold + 1))\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train = eval('fold' + str(ii + 1)) + train\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train_mask = eval('fold_mask' + str(ii + 1)) + train_mask\n","\n","    for ii in range(opts['k_fold']):\n","        if ii != K_fold:\n","            train_dis = eval('fold_dis' + str(ii + 1)) + train_dis\n","\n","    if opts['k_fold'] == 1: # for no cross validation the training will be with all training images\n","        train = train_files\n","        train_mask = train_files_mask\n","        train_dis = train_files_dis\n","\n","    random.Random(opts['random_seed_num']).shuffle(train)\n","    random.Random(opts['random_seed_num']).shuffle(train_mask)\n","    random.Random(opts['random_seed_num']).shuffle(train_dis)\n","\n","\n","    ## creating validation data for each fold (just for evaluation)\n","    # it is not included in the main training loop for a faster training\n","    validation_X = []\n","    validation_Y = []\n","    validation_DIS = []\n","    if len(val)<200: # memory consideration\n","        for an in range(len(val)):\n","            x = cv2.imread(val[an])\n","            x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","\n","            aug = albumentation_aug_light(1, opts['crop_size'], opts['crop_size'])\n","            #augmented = aug(image=x)\n","            #x = augmented['image']\n","            if opts['use_pretrained_flag'] == 1:\n","                x = preprocess_input(x)\n","            img_mask = imread(val_label[an])\n","            x = x/255\n","            validation_X.append(x)\n","            validation_Y.append(img_mask)\n","\n","    else:\n","        for an in range(200):\n","            x = cv2.imread(val[an])\n","            x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n","\n","            aug = albumentation_aug_light(1, opts['crop_size'], opts['crop_size'])\n","            #augmented = aug(image=x)\n","            #x = augmented['image']\n","            if opts['use_pretrained_flag'] ==1:\n","                x = preprocess_input(x)\n","            img_mask = imread(val_label[an])\n","            x = x/255\n","            validation_X.append(x)\n","            validation_Y.append(img_mask)\n","\n","    validation_X = np.array(validation_X)\n","    validation_Y = np.array(validation_Y)\n","\n","\n","    model_path = opts['models_save_path'] + 'raw_unet_{}.h5'.format(K_fold+1)\n","    logger = CSVLogger(opts['models_save_path']+ 'raw_unet_{}.log'.format(K_fold + 1))\n","    LR_drop = step_decay_schedule(initial_lr= opts['init_LR'], decay_factor = opts['LR_decay_factor'], epochs_drop = opts['LR_drop_after_nth_epoch'])\n","    model_raw = deeper_binary_unet(opts['number_of_channel'], opts['init_LR'])\n","    checkpoint = ModelCheckpoint(model_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max', save_weights_only = True)\n","\n","    # training\n","    history = model_raw.fit_generator(data_gen_heavy(train,\n","                                                     train_mask,\n","                                                     opts['batch_size'],\n","                                                     1,\n","                                                     opts['crop_size'], opts['crop_size'],\n","                                                     distance_unet_flag=0,\n","                                                     augment=True,\n","                                                     BACKBONE_model=opts['pretrained_model'],\n","                                                     use_pretrain_flag=opts['use_pretrained_flag']),\n","                                      validation_data=data_gen_heavy(val,\n","                                                                     val_mask,\n","                                                                     opts['batch_size'],\n","                                                                     1,\n","                                                                     opts['crop_size'], opts['crop_size'],\n","                                                                     distance_unet_flag=0,\n","                                                                     augment=True,\n","                                                                     BACKBONE_model=opts['pretrained_model'],\n","                                                                     use_pretrain_flag=opts['use_pretrained_flag']),\n","                                      validation_steps=1,\n","                                      epochs=opts['epoch_num_stage1'], verbose=1,\n","                                      callbacks=[checkpoint, logger, LR_drop],\n","                                      steps_per_epoch=(len(train) // opts['batch_size']) // opts['quick_run'])\n","\n","    model_raw.load_weights(opts['models_save_path'] + 'raw_unet_{}.h5'.format(K_fold + 1))\n","\n","    ## predication on validation set\n","    preds_val = model_raw.predict(validation_X, verbose=1, batch_size=1)\n","    preds_val_t = (preds_val > opts['treshold']).astype(np.uint8)\n","\n","\n","    for val_len in range(len(preds_val)):\n","        # with watershed post processing\n","        local_maxi = peak_local_max(np.squeeze(preds_val[val_len]), indices=False,exclude_border=False, footprint=np.ones((15, 15)))\n","        markers = ndi.label(local_maxi)[0]\n","        labels = watershed(-np.squeeze(preds_val[val_len]), markers,mask = np.squeeze(preds_val_t[[val_len]]))\n","        labels[np.squeeze(preds_val_t[[val_len]])==0] = 0\n","\n","        # without post processing\n","        pred = np.squeeze(preds_val_t[val_len])\n","        label_pred = skimage.morphology.label(pred)\n","\n","        label_pred = remap_label(label_pred)\n","        validation_Y[val_len] = remap_label(validation_Y[val_len])\n","        labels = remap_label(labels)\n","\n","        imsave(opts['results_save_path'] + 'stage1/validation/watershed_unet/{}.png'.format(get_id_from_file_path(val[val_len], opts['imageType_train'])),labels.astype(np.uint16))\n","        imsave(opts['results_save_path'] + 'stage1/validation/pure_unet/{}.png'.format(get_id_from_file_path(val[val_len], opts['imageType_train'])),label_pred.astype(np.uint16))\n","\n","\n","\n","        dice_pure_unet[K_fold, val_len]= get_dice_1(validation_Y[val_len], label_pred)\n","        AJI_pure_unet[K_fold, val_len] = get_fast_aji(validation_Y[val_len], label_pred,)\n","        PQ_pure_unet[K_fold, val_len] = get_fast_pq(validation_Y[val_len], label_pred,)[0][2]\n","\n","        dice_unet_watershed[K_fold, val_len]= get_dice_1(validation_Y[val_len],labels, )\n","        AJI_unet_watershed[K_fold, val_len] = get_fast_aji(validation_Y[val_len], labels)\n","        PQ_unet_watershed[K_fold, val_len]  = get_fast_pq(validation_Y[val_len], labels)[0][2]\n","\n","\n","    print('==========')\n","    print('average dice pure Unet for fold{}:'.format(K_fold), np.mean(dice_pure_unet[K_fold, :]))\n","    print('average AJI pure Unet for fold{}:'.format(K_fold), np.mean(AJI_pure_unet[K_fold, :]))\n","    print('average PQ pure Unet for fold{}:'.format(K_fold), np.mean(PQ_pure_unet[K_fold, :]))\n","\n","    print('==========')\n","\n","    print('==========')\n","    print('average Dice Unet watershed for fold{}:'.format(K_fold), np.mean(dice_unet_watershed[K_fold, :]))\n","    print('average AJI Unet watershed for fold{}:'.format(K_fold), np.mean(AJI_unet_watershed[K_fold, :]))\n","    print('average PQ Unet watershed for fold{}:'.format(K_fold), np.mean(PQ_unet_watershed[K_fold, :]))\n","    print('==========')\n","finish_time = time.time()\n","print('==========')\n","print('total training time (all 10 folds):',  (finish_time- start_time)/60, 'minutes')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:22:00.103325Z","iopub.execute_input":"2024-04-28T15:22:00.103581Z","iopub.status.idle":"2024-04-28T17:19:04.05373Z","shell.execute_reply.started":"2024-04-28T15:22:00.103556Z","shell.execute_reply":"2024-04-28T17:19:04.05272Z"},"trusted":true,"id":"quoCLwU30y1d","outputId":"d6d05660-8451-4380-eaf0-602dc9903530"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 47s 473ms/step - loss: -0.4133 - dice_coef: 0.6221 - val_loss: -0.4689 - val_dice_coef: 0.6028\n\nEpoch 00001: val_dice_coef improved from -inf to 0.60283, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 37s 372ms/step - loss: -0.5330 - dice_coef: 0.7149 - val_loss: -0.6074 - val_dice_coef: 0.6831\n\nEpoch 00002: val_dice_coef improved from 0.60283 to 0.68305, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 361ms/step - loss: -0.5863 - dice_coef: 0.7469 - val_loss: -0.5318 - val_dice_coef: 0.6534\n\nEpoch 00003: val_dice_coef did not improve from 0.68305\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 360ms/step - loss: -0.6108 - dice_coef: 0.7615 - val_loss: -0.6388 - val_dice_coef: 0.7198\n\nEpoch 00004: val_dice_coef improved from 0.68305 to 0.71983, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 361ms/step - loss: -0.6365 - dice_coef: 0.7775 - val_loss: -0.6493 - val_dice_coef: 0.7231\n\nEpoch 00005: val_dice_coef improved from 0.71983 to 0.72308, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 353ms/step - loss: -0.6476 - dice_coef: 0.7845 - val_loss: -0.6683 - val_dice_coef: 0.7514\n\nEpoch 00006: val_dice_coef improved from 0.72308 to 0.75143, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 366ms/step - loss: -0.6686 - dice_coef: 0.7976 - val_loss: -0.6814 - val_dice_coef: 0.7527\n\nEpoch 00007: val_dice_coef improved from 0.75143 to 0.75268, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 367ms/step - loss: -0.6915 - dice_coef: 0.8121 - val_loss: -0.6512 - val_dice_coef: 0.7395\n\nEpoch 00008: val_dice_coef did not improve from 0.75268\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 37s 370ms/step - loss: -0.7200 - dice_coef: 0.8299 - val_loss: -0.7014 - val_dice_coef: 0.7778\n\nEpoch 00009: val_dice_coef improved from 0.75268 to 0.77783, saving model to /kaggle/working/models/raw_unet_1.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 37s 375ms/step - loss: -0.7387 - dice_coef: 0.8416 - val_loss: -0.6556 - val_dice_coef: 0.7508\n\nEpoch 00010: val_dice_coef did not improve from 0.77783\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 37s 377ms/step - loss: -0.7484 - dice_coef: 0.8475 - val_loss: -0.6818 - val_dice_coef: 0.7681\n\nEpoch 00011: val_dice_coef did not improve from 0.77783\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 364ms/step - loss: -0.7596 - dice_coef: 0.8544 - val_loss: -0.6357 - val_dice_coef: 0.7430\n\nEpoch 00012: val_dice_coef did not improve from 0.77783\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 38s 380ms/step - loss: -0.7682 - dice_coef: 0.8595 - val_loss: -0.6306 - val_dice_coef: 0.7437\n\nEpoch 00013: val_dice_coef did not improve from 0.77783\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 360ms/step - loss: -0.7793 - dice_coef: 0.8663 - val_loss: -0.6090 - val_dice_coef: 0.7375\n\nEpoch 00014: val_dice_coef did not improve from 0.77783\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 361ms/step - loss: -0.7847 - dice_coef: 0.8696 - val_loss: -0.6555 - val_dice_coef: 0.7638\n\nEpoch 00015: val_dice_coef did not improve from 0.77783\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7914 - dice_coef: 0.8737 - val_loss: -0.6303 - val_dice_coef: 0.7534\n\nEpoch 00016: val_dice_coef did not improve from 0.77783\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 351ms/step - loss: -0.7996 - dice_coef: 0.8784 - val_loss: -0.5955 - val_dice_coef: 0.7375\n\nEpoch 00017: val_dice_coef did not improve from 0.77783\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 354ms/step - loss: -0.8052 - dice_coef: 0.8818 - val_loss: -0.6407 - val_dice_coef: 0.7600\n\nEpoch 00018: val_dice_coef did not improve from 0.77783\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 358ms/step - loss: -0.8093 - dice_coef: 0.8843 - val_loss: -0.6512 - val_dice_coef: 0.7653\n\nEpoch 00019: val_dice_coef did not improve from 0.77783\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 357ms/step - loss: -0.8120 - dice_coef: 0.8859 - val_loss: -0.6657 - val_dice_coef: 0.7731\n\nEpoch 00020: val_dice_coef did not improve from 0.77783\n3/3 [==============================] - 1s 168ms/step\n==========\naverage dice pure Unet for fold0: 0.7937770196499847\naverage AJI pure Unet for fold0: 0.5123998921816139\naverage PQ pure Unet for fold0: 0.4733178071300849\n==========\n==========\naverage Dice Unet watershed for fold0: 0.793851995160311\naverage AJI Unet watershed for fold0: 0.5298151296196946\naverage PQ Unet watershed for fold0: 0.46827500736435557\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 361ms/step - loss: -0.4473 - dice_coef: 0.6394 - val_loss: -0.4676 - val_dice_coef: 0.6786\n\nEpoch 00001: val_dice_coef improved from -inf to 0.67858, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 332ms/step - loss: -0.5559 - dice_coef: 0.7224 - val_loss: -0.5073 - val_dice_coef: 0.7223\n\nEpoch 00002: val_dice_coef improved from 0.67858 to 0.72235, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 335ms/step - loss: -0.5922 - dice_coef: 0.7457 - val_loss: -0.5267 - val_dice_coef: 0.7313\n\nEpoch 00003: val_dice_coef improved from 0.72235 to 0.73131, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 336ms/step - loss: -0.6321 - dice_coef: 0.7707 - val_loss: -0.5682 - val_dice_coef: 0.7583\n\nEpoch 00004: val_dice_coef improved from 0.73131 to 0.75825, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 338ms/step - loss: -0.6539 - dice_coef: 0.7845 - val_loss: -0.5543 - val_dice_coef: 0.7494\n\nEpoch 00005: val_dice_coef did not improve from 0.75825\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 336ms/step - loss: -0.6761 - dice_coef: 0.7987 - val_loss: -0.5665 - val_dice_coef: 0.7545\n\nEpoch 00006: val_dice_coef did not improve from 0.75825\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 335ms/step - loss: -0.7014 - dice_coef: 0.8149 - val_loss: -0.5497 - val_dice_coef: 0.7503\n\nEpoch 00007: val_dice_coef did not improve from 0.75825\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 333ms/step - loss: -0.7280 - dice_coef: 0.8320 - val_loss: -0.6524 - val_dice_coef: 0.7996\n\nEpoch 00008: val_dice_coef improved from 0.75825 to 0.79960, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 334ms/step - loss: -0.7550 - dice_coef: 0.8486 - val_loss: -0.5271 - val_dice_coef: 0.7602\n\nEpoch 00009: val_dice_coef did not improve from 0.79960\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 333ms/step - loss: -0.7695 - dice_coef: 0.8579 - val_loss: -0.6434 - val_dice_coef: 0.8027\n\nEpoch 00010: val_dice_coef improved from 0.79960 to 0.80270, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 334ms/step - loss: -0.7798 - dice_coef: 0.8641 - val_loss: -0.6113 - val_dice_coef: 0.7909\n\nEpoch 00011: val_dice_coef did not improve from 0.80270\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 334ms/step - loss: -0.7901 - dice_coef: 0.8704 - val_loss: -0.6232 - val_dice_coef: 0.8040\n\nEpoch 00012: val_dice_coef improved from 0.80270 to 0.80398, saving model to /kaggle/working/models/raw_unet_2.h5\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.7985 - dice_coef: 0.8756 - val_loss: -0.5915 - val_dice_coef: 0.7941\n\nEpoch 00013: val_dice_coef did not improve from 0.80398\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 333ms/step - loss: -0.8053 - dice_coef: 0.8797 - val_loss: -0.5891 - val_dice_coef: 0.7912\n\nEpoch 00014: val_dice_coef did not improve from 0.80398\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8117 - dice_coef: 0.8836 - val_loss: -0.5755 - val_dice_coef: 0.7889\n\nEpoch 00015: val_dice_coef did not improve from 0.80398\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 334ms/step - loss: -0.8183 - dice_coef: 0.8877 - val_loss: -0.5728 - val_dice_coef: 0.7841\n\nEpoch 00016: val_dice_coef did not improve from 0.80398\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8252 - dice_coef: 0.8918 - val_loss: -0.5742 - val_dice_coef: 0.7908\n\nEpoch 00017: val_dice_coef did not improve from 0.80398\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 333ms/step - loss: -0.8297 - dice_coef: 0.8946 - val_loss: -0.5690 - val_dice_coef: 0.7884\n\nEpoch 00018: val_dice_coef did not improve from 0.80398\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8320 - dice_coef: 0.8961 - val_loss: -0.5638 - val_dice_coef: 0.7896\n\nEpoch 00019: val_dice_coef did not improve from 0.80398\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 334ms/step - loss: -0.8361 - dice_coef: 0.8986 - val_loss: -0.5589 - val_dice_coef: 0.7860\n\nEpoch 00020: val_dice_coef did not improve from 0.80398\n3/3 [==============================] - 0s 70ms/step\n==========\naverage dice pure Unet for fold1: 0.8066933339014346\naverage AJI pure Unet for fold1: 0.4662709630885887\naverage PQ pure Unet for fold1: 0.433086304751859\n==========\n==========\naverage Dice Unet watershed for fold1: 0.8062953274924123\naverage AJI Unet watershed for fold1: 0.5170818321085161\naverage PQ Unet watershed for fold1: 0.43624456172308984\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 37s 378ms/step - loss: -0.4609 - dice_coef: 0.6453 - val_loss: -0.2626 - val_dice_coef: 0.6570\n\nEpoch 00001: val_dice_coef improved from -inf to 0.65696, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 351ms/step - loss: -0.5765 - dice_coef: 0.7319 - val_loss: -0.2535 - val_dice_coef: 0.6363\n\nEpoch 00002: val_dice_coef did not improve from 0.65696\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 354ms/step - loss: -0.5982 - dice_coef: 0.7445 - val_loss: -0.4601 - val_dice_coef: 0.7311\n\nEpoch 00003: val_dice_coef improved from 0.65696 to 0.73111, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.4880 - dice_coef: 0.6891 - val_loss: -0.2378 - val_dice_coef: 0.6590\n\nEpoch 00004: val_dice_coef did not improve from 0.73111\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 354ms/step - loss: -0.5400 - dice_coef: 0.7074 - val_loss: -0.3974 - val_dice_coef: 0.7043\n\nEpoch 00005: val_dice_coef did not improve from 0.73111\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.5777 - dice_coef: 0.7316 - val_loss: -0.3885 - val_dice_coef: 0.7188\n\nEpoch 00006: val_dice_coef did not improve from 0.73111\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.6066 - dice_coef: 0.7504 - val_loss: -0.2430 - val_dice_coef: 0.6584\n\nEpoch 00007: val_dice_coef did not improve from 0.73111\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 32s 327ms/step - loss: -0.6302 - dice_coef: 0.7657 - val_loss: -0.4307 - val_dice_coef: 0.7248\n\nEpoch 00008: val_dice_coef did not improve from 0.73111\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 328ms/step - loss: -0.6495 - dice_coef: 0.7780 - val_loss: -0.4719 - val_dice_coef: 0.7528\n\nEpoch 00009: val_dice_coef improved from 0.73111 to 0.75283, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 327ms/step - loss: -0.6571 - dice_coef: 0.7832 - val_loss: -0.5317 - val_dice_coef: 0.7708\n\nEpoch 00010: val_dice_coef improved from 0.75283 to 0.77076, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 328ms/step - loss: -0.6670 - dice_coef: 0.7892 - val_loss: -0.4826 - val_dice_coef: 0.7522\n\nEpoch 00011: val_dice_coef did not improve from 0.77076\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 327ms/step - loss: -0.6755 - dice_coef: 0.7948 - val_loss: -0.5176 - val_dice_coef: 0.7769\n\nEpoch 00012: val_dice_coef improved from 0.77076 to 0.77694, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 328ms/step - loss: -0.6946 - dice_coef: 0.8070 - val_loss: -0.5502 - val_dice_coef: 0.7824\n\nEpoch 00013: val_dice_coef improved from 0.77694 to 0.78241, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 327ms/step - loss: -0.7094 - dice_coef: 0.8167 - val_loss: -0.4893 - val_dice_coef: 0.7731\n\nEpoch 00014: val_dice_coef did not improve from 0.78241\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 328ms/step - loss: -0.7217 - dice_coef: 0.8245 - val_loss: -0.5273 - val_dice_coef: 0.7807\n\nEpoch 00015: val_dice_coef did not improve from 0.78241\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 32s 327ms/step - loss: -0.7330 - dice_coef: 0.8319 - val_loss: -0.4320 - val_dice_coef: 0.7570\n\nEpoch 00016: val_dice_coef did not improve from 0.78241\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 32s 327ms/step - loss: -0.7433 - dice_coef: 0.8384 - val_loss: -0.5499 - val_dice_coef: 0.7917\n\nEpoch 00017: val_dice_coef improved from 0.78241 to 0.79171, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 32s 327ms/step - loss: -0.7545 - dice_coef: 0.8455 - val_loss: -0.5117 - val_dice_coef: 0.7884\n\nEpoch 00018: val_dice_coef did not improve from 0.79171\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 32s 328ms/step - loss: -0.7602 - dice_coef: 0.8491 - val_loss: -0.5480 - val_dice_coef: 0.8020\n\nEpoch 00019: val_dice_coef improved from 0.79171 to 0.80198, saving model to /kaggle/working/models/raw_unet_3.h5\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 32s 327ms/step - loss: -0.7659 - dice_coef: 0.8527 - val_loss: -0.5426 - val_dice_coef: 0.8010\n\nEpoch 00020: val_dice_coef did not improve from 0.80198\n3/3 [==============================] - 0s 69ms/step\n==========\naverage dice pure Unet for fold2: 0.7995310131536523\naverage AJI pure Unet for fold2: 0.28643896882992576\naverage PQ pure Unet for fold2: 0.31044080657489864\n==========\n==========\naverage Dice Unet watershed for fold2: 0.7984862671097327\naverage AJI Unet watershed for fold2: 0.4936727470067517\naverage PQ Unet watershed for fold2: 0.4445369360972861\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 366ms/step - loss: -0.3875 - dice_coef: 0.5987 - val_loss: -0.5639 - val_dice_coef: 0.7413\n\nEpoch 00001: val_dice_coef improved from -inf to 0.74126, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 339ms/step - loss: -0.5071 - dice_coef: 0.6869 - val_loss: -0.5887 - val_dice_coef: 0.7754\n\nEpoch 00002: val_dice_coef improved from 0.74126 to 0.77545, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 339ms/step - loss: -0.5734 - dice_coef: 0.7297 - val_loss: -0.6342 - val_dice_coef: 0.7995\n\nEpoch 00003: val_dice_coef improved from 0.77545 to 0.79954, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 342ms/step - loss: -0.6108 - dice_coef: 0.7540 - val_loss: -0.6608 - val_dice_coef: 0.8138\n\nEpoch 00004: val_dice_coef improved from 0.79954 to 0.81377, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 344ms/step - loss: -0.6311 - dice_coef: 0.7666 - val_loss: -0.6106 - val_dice_coef: 0.7857\n\nEpoch 00005: val_dice_coef did not improve from 0.81377\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 338ms/step - loss: -0.6496 - dice_coef: 0.7786 - val_loss: -0.6442 - val_dice_coef: 0.8095\n\nEpoch 00006: val_dice_coef did not improve from 0.81377\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 339ms/step - loss: -0.6542 - dice_coef: 0.7810 - val_loss: -0.6722 - val_dice_coef: 0.8224\n\nEpoch 00007: val_dice_coef improved from 0.81377 to 0.82245, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 338ms/step - loss: -0.6663 - dice_coef: 0.7889 - val_loss: -0.6610 - val_dice_coef: 0.8118\n\nEpoch 00008: val_dice_coef did not improve from 0.82245\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 338ms/step - loss: -0.6820 - dice_coef: 0.7990 - val_loss: -0.6689 - val_dice_coef: 0.8239\n\nEpoch 00009: val_dice_coef improved from 0.82245 to 0.82392, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 345ms/step - loss: -0.6883 - dice_coef: 0.8031 - val_loss: -0.6696 - val_dice_coef: 0.8241\n\nEpoch 00010: val_dice_coef improved from 0.82392 to 0.82410, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 338ms/step - loss: -0.6976 - dice_coef: 0.8093 - val_loss: -0.6639 - val_dice_coef: 0.8280\n\nEpoch 00011: val_dice_coef improved from 0.82410 to 0.82804, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 346ms/step - loss: -0.7083 - dice_coef: 0.8163 - val_loss: -0.6629 - val_dice_coef: 0.8237\n\nEpoch 00012: val_dice_coef did not improve from 0.82804\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 345ms/step - loss: -0.7154 - dice_coef: 0.8210 - val_loss: -0.6780 - val_dice_coef: 0.8313\n\nEpoch 00013: val_dice_coef improved from 0.82804 to 0.83129, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 339ms/step - loss: -0.7258 - dice_coef: 0.8279 - val_loss: -0.6518 - val_dice_coef: 0.8203\n\nEpoch 00014: val_dice_coef did not improve from 0.83129\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 340ms/step - loss: -0.7378 - dice_coef: 0.8358 - val_loss: -0.6748 - val_dice_coef: 0.8336\n\nEpoch 00015: val_dice_coef improved from 0.83129 to 0.83361, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 340ms/step - loss: -0.7508 - dice_coef: 0.8442 - val_loss: -0.6676 - val_dice_coef: 0.8340\n\nEpoch 00016: val_dice_coef improved from 0.83361 to 0.83401, saving model to /kaggle/working/models/raw_unet_4.h5\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 339ms/step - loss: -0.7636 - dice_coef: 0.8522 - val_loss: -0.6585 - val_dice_coef: 0.8271\n\nEpoch 00017: val_dice_coef did not improve from 0.83401\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 345ms/step - loss: -0.7690 - dice_coef: 0.8557 - val_loss: -0.6583 - val_dice_coef: 0.8289\n\nEpoch 00018: val_dice_coef did not improve from 0.83401\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 345ms/step - loss: -0.7737 - dice_coef: 0.8587 - val_loss: -0.6599 - val_dice_coef: 0.8291\n\nEpoch 00019: val_dice_coef did not improve from 0.83401\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 339ms/step - loss: -0.7788 - dice_coef: 0.8619 - val_loss: -0.6113 - val_dice_coef: 0.8134\n\nEpoch 00020: val_dice_coef did not improve from 0.83401\n3/3 [==============================] - 0s 69ms/step\n==========\naverage dice pure Unet for fold3: 0.8420304817952359\naverage AJI pure Unet for fold3: 0.3140332838122161\naverage PQ pure Unet for fold3: 0.3410839524368246\n==========\n==========\naverage Dice Unet watershed for fold3: 0.8395040161997861\naverage AJI Unet watershed for fold3: 0.5218395694244725\naverage PQ Unet watershed for fold3: 0.4667255750412258\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 367ms/step - loss: -0.3982 - dice_coef: 0.6089 - val_loss: -0.3324 - val_dice_coef: 0.5494\n\nEpoch 00001: val_dice_coef improved from -inf to 0.54944, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 336ms/step - loss: -0.5684 - dice_coef: 0.7366 - val_loss: -0.4578 - val_dice_coef: 0.6159\n\nEpoch 00002: val_dice_coef improved from 0.54944 to 0.61592, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 338ms/step - loss: -0.6259 - dice_coef: 0.7721 - val_loss: -0.4762 - val_dice_coef: 0.6267\n\nEpoch 00003: val_dice_coef improved from 0.61592 to 0.62668, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 336ms/step - loss: -0.6537 - dice_coef: 0.7890 - val_loss: -0.5186 - val_dice_coef: 0.6615\n\nEpoch 00004: val_dice_coef improved from 0.62668 to 0.66153, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 336ms/step - loss: -0.6803 - dice_coef: 0.8053 - val_loss: -0.4970 - val_dice_coef: 0.6425\n\nEpoch 00005: val_dice_coef did not improve from 0.66153\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 335ms/step - loss: -0.7103 - dice_coef: 0.8244 - val_loss: -0.5242 - val_dice_coef: 0.6628\n\nEpoch 00006: val_dice_coef improved from 0.66153 to 0.66278, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 337ms/step - loss: -0.7407 - dice_coef: 0.8428 - val_loss: -0.4770 - val_dice_coef: 0.6502\n\nEpoch 00007: val_dice_coef did not improve from 0.66278\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 33s 335ms/step - loss: -0.7628 - dice_coef: 0.8563 - val_loss: -0.5496 - val_dice_coef: 0.6941\n\nEpoch 00008: val_dice_coef improved from 0.66278 to 0.69408, saving model to /kaggle/working/models/raw_unet_5.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.7809 - dice_coef: 0.8673 - val_loss: -0.4753 - val_dice_coef: 0.6538\n\nEpoch 00009: val_dice_coef did not improve from 0.69408\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 335ms/step - loss: -0.7927 - dice_coef: 0.8743 - val_loss: -0.4675 - val_dice_coef: 0.6526\n\nEpoch 00010: val_dice_coef did not improve from 0.69408\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8002 - dice_coef: 0.8787 - val_loss: -0.4831 - val_dice_coef: 0.6655\n\nEpoch 00011: val_dice_coef did not improve from 0.69408\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8099 - dice_coef: 0.8846 - val_loss: -0.4826 - val_dice_coef: 0.6710\n\nEpoch 00012: val_dice_coef did not improve from 0.69408\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8175 - dice_coef: 0.8891 - val_loss: -0.5062 - val_dice_coef: 0.6753\n\nEpoch 00013: val_dice_coef did not improve from 0.69408\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8242 - dice_coef: 0.8931 - val_loss: -0.4686 - val_dice_coef: 0.6689\n\nEpoch 00014: val_dice_coef did not improve from 0.69408\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8306 - dice_coef: 0.8969 - val_loss: -0.4827 - val_dice_coef: 0.6728\n\nEpoch 00015: val_dice_coef did not improve from 0.69408\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8351 - dice_coef: 0.8997 - val_loss: -0.4695 - val_dice_coef: 0.6693\n\nEpoch 00016: val_dice_coef did not improve from 0.69408\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8453 - dice_coef: 0.9058 - val_loss: -0.4655 - val_dice_coef: 0.6681\n\nEpoch 00017: val_dice_coef did not improve from 0.69408\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 336ms/step - loss: -0.8494 - dice_coef: 0.9084 - val_loss: -0.4461 - val_dice_coef: 0.6632\n\nEpoch 00018: val_dice_coef did not improve from 0.69408\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 337ms/step - loss: -0.8532 - dice_coef: 0.9107 - val_loss: -0.4998 - val_dice_coef: 0.6861\n\nEpoch 00019: val_dice_coef did not improve from 0.69408\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 335ms/step - loss: -0.8572 - dice_coef: 0.9132 - val_loss: -0.5135 - val_dice_coef: 0.6933\n\nEpoch 00020: val_dice_coef did not improve from 0.69408\n3/3 [==============================] - 0s 70ms/step\n==========\naverage dice pure Unet for fold4: 0.713950027385137\naverage AJI pure Unet for fold4: 0.32630993163814437\naverage PQ pure Unet for fold4: 0.27042623959475703\n==========\n==========\naverage Dice Unet watershed for fold4: 0.7138014646514259\naverage AJI Unet watershed for fold4: 0.3957915557582477\naverage PQ Unet watershed for fold4: 0.30883303020295044\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 38s 381ms/step - loss: -0.3538 - dice_coef: 0.5776 - val_loss: -0.4441 - val_dice_coef: 0.5869\n\nEpoch 00001: val_dice_coef improved from -inf to 0.58691, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.5280 - dice_coef: 0.7089 - val_loss: -0.5121 - val_dice_coef: 0.6251\n\nEpoch 00002: val_dice_coef improved from 0.58691 to 0.62511, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.5840 - dice_coef: 0.7436 - val_loss: -0.4941 - val_dice_coef: 0.6199\n\nEpoch 00003: val_dice_coef did not improve from 0.62511\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 353ms/step - loss: -0.6215 - dice_coef: 0.7676 - val_loss: -0.5372 - val_dice_coef: 0.6499\n\nEpoch 00004: val_dice_coef improved from 0.62511 to 0.64990, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.6473 - dice_coef: 0.7835 - val_loss: -0.5113 - val_dice_coef: 0.6478\n\nEpoch 00005: val_dice_coef did not improve from 0.64990\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 348ms/step - loss: -0.6655 - dice_coef: 0.7949 - val_loss: -0.5964 - val_dice_coef: 0.6993\n\nEpoch 00006: val_dice_coef improved from 0.64990 to 0.69934, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.6812 - dice_coef: 0.8047 - val_loss: -0.5770 - val_dice_coef: 0.6866\n\nEpoch 00007: val_dice_coef did not improve from 0.69934\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.6921 - dice_coef: 0.8115 - val_loss: -0.6128 - val_dice_coef: 0.7153\n\nEpoch 00008: val_dice_coef improved from 0.69934 to 0.71526, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 356ms/step - loss: -0.7145 - dice_coef: 0.8256 - val_loss: -0.5938 - val_dice_coef: 0.7126\n\nEpoch 00009: val_dice_coef did not improve from 0.71526\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7246 - dice_coef: 0.8320 - val_loss: -0.5883 - val_dice_coef: 0.7119\n\nEpoch 00010: val_dice_coef did not improve from 0.71526\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 356ms/step - loss: -0.7332 - dice_coef: 0.8374 - val_loss: -0.5906 - val_dice_coef: 0.7128\n\nEpoch 00011: val_dice_coef did not improve from 0.71526\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 353ms/step - loss: -0.7410 - dice_coef: 0.8424 - val_loss: -0.6146 - val_dice_coef: 0.7329\n\nEpoch 00012: val_dice_coef improved from 0.71526 to 0.73292, saving model to /kaggle/working/models/raw_unet_6.h5\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 358ms/step - loss: -0.7455 - dice_coef: 0.8451 - val_loss: -0.6147 - val_dice_coef: 0.7317\n\nEpoch 00013: val_dice_coef did not improve from 0.73292\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 352ms/step - loss: -0.7526 - dice_coef: 0.8495 - val_loss: -0.5401 - val_dice_coef: 0.6924\n\nEpoch 00014: val_dice_coef did not improve from 0.73292\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 355ms/step - loss: -0.7618 - dice_coef: 0.8552 - val_loss: -0.5813 - val_dice_coef: 0.7189\n\nEpoch 00015: val_dice_coef did not improve from 0.73292\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 355ms/step - loss: -0.7682 - dice_coef: 0.8590 - val_loss: -0.5733 - val_dice_coef: 0.7219\n\nEpoch 00016: val_dice_coef did not improve from 0.73292\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 355ms/step - loss: -0.7715 - dice_coef: 0.8611 - val_loss: -0.5925 - val_dice_coef: 0.7255\n\nEpoch 00017: val_dice_coef did not improve from 0.73292\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 353ms/step - loss: -0.7725 - dice_coef: 0.8618 - val_loss: -0.5838 - val_dice_coef: 0.7214\n\nEpoch 00018: val_dice_coef did not improve from 0.73292\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 355ms/step - loss: -0.7766 - dice_coef: 0.8642 - val_loss: -0.5874 - val_dice_coef: 0.7294\n\nEpoch 00019: val_dice_coef did not improve from 0.73292\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 353ms/step - loss: -0.7808 - dice_coef: 0.8667 - val_loss: -0.5971 - val_dice_coef: 0.7314\n\nEpoch 00020: val_dice_coef did not improve from 0.73292\n3/3 [==============================] - 0s 73ms/step\n==========\naverage dice pure Unet for fold5: 0.7449807364940387\naverage AJI pure Unet for fold5: 0.423141119813183\naverage PQ pure Unet for fold5: 0.3603671582218488\n==========\n==========\naverage Dice Unet watershed for fold5: 0.7452195746487176\naverage AJI Unet watershed for fold5: 0.4401726260339672\naverage PQ Unet watershed for fold5: 0.3669768200515735\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 37s 377ms/step - loss: -0.3993 - dice_coef: 0.6051 - val_loss: -0.4367 - val_dice_coef: 0.6329\n\nEpoch 00001: val_dice_coef improved from -inf to 0.63290, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 347ms/step - loss: -0.5395 - dice_coef: 0.7161 - val_loss: -0.5302 - val_dice_coef: 0.6759\n\nEpoch 00002: val_dice_coef improved from 0.63290 to 0.67592, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.5751 - dice_coef: 0.7379 - val_loss: -0.5713 - val_dice_coef: 0.6916\n\nEpoch 00003: val_dice_coef improved from 0.67592 to 0.69163, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 343ms/step - loss: -0.6157 - dice_coef: 0.7629 - val_loss: -0.5786 - val_dice_coef: 0.7107\n\nEpoch 00004: val_dice_coef improved from 0.69163 to 0.71070, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.6533 - dice_coef: 0.7868 - val_loss: -0.5819 - val_dice_coef: 0.7043\n\nEpoch 00005: val_dice_coef did not improve from 0.71070\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 347ms/step - loss: -0.6659 - dice_coef: 0.7947 - val_loss: -0.6135 - val_dice_coef: 0.7356\n\nEpoch 00006: val_dice_coef improved from 0.71070 to 0.73565, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 347ms/step - loss: -0.6918 - dice_coef: 0.8107 - val_loss: -0.6010 - val_dice_coef: 0.7249\n\nEpoch 00007: val_dice_coef did not improve from 0.73565\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 347ms/step - loss: -0.7024 - dice_coef: 0.8177 - val_loss: -0.6198 - val_dice_coef: 0.7419\n\nEpoch 00008: val_dice_coef improved from 0.73565 to 0.74185, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 346ms/step - loss: -0.7238 - dice_coef: 0.8311 - val_loss: -0.6145 - val_dice_coef: 0.7473\n\nEpoch 00009: val_dice_coef improved from 0.74185 to 0.74731, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 347ms/step - loss: -0.7343 - dice_coef: 0.8376 - val_loss: -0.6077 - val_dice_coef: 0.7496\n\nEpoch 00010: val_dice_coef improved from 0.74731 to 0.74958, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 351ms/step - loss: -0.7432 - dice_coef: 0.8433 - val_loss: -0.5714 - val_dice_coef: 0.7280\n\nEpoch 00011: val_dice_coef did not improve from 0.74958\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7504 - dice_coef: 0.8478 - val_loss: -0.5860 - val_dice_coef: 0.7391\n\nEpoch 00012: val_dice_coef did not improve from 0.74958\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 347ms/step - loss: -0.7557 - dice_coef: 0.8509 - val_loss: -0.6076 - val_dice_coef: 0.7484\n\nEpoch 00013: val_dice_coef did not improve from 0.74958\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 347ms/step - loss: -0.7636 - dice_coef: 0.8558 - val_loss: -0.6053 - val_dice_coef: 0.7476\n\nEpoch 00014: val_dice_coef did not improve from 0.74958\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7693 - dice_coef: 0.8593 - val_loss: -0.5758 - val_dice_coef: 0.7371\n\nEpoch 00015: val_dice_coef did not improve from 0.74958\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 347ms/step - loss: -0.7737 - dice_coef: 0.8620 - val_loss: -0.5883 - val_dice_coef: 0.7433\n\nEpoch 00016: val_dice_coef did not improve from 0.74958\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 354ms/step - loss: -0.7816 - dice_coef: 0.8667 - val_loss: -0.5581 - val_dice_coef: 0.7290\n\nEpoch 00017: val_dice_coef did not improve from 0.74958\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7847 - dice_coef: 0.8686 - val_loss: -0.6032 - val_dice_coef: 0.7549\n\nEpoch 00018: val_dice_coef improved from 0.74958 to 0.75488, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 348ms/step - loss: -0.7879 - dice_coef: 0.8705 - val_loss: -0.6035 - val_dice_coef: 0.7570\n\nEpoch 00019: val_dice_coef improved from 0.75488 to 0.75702, saving model to /kaggle/working/models/raw_unet_7.h5\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 337ms/step - loss: -0.7911 - dice_coef: 0.8725 - val_loss: -0.5959 - val_dice_coef: 0.7518\n\nEpoch 00020: val_dice_coef did not improve from 0.75702\n3/3 [==============================] - 0s 71ms/step\n==========\naverage dice pure Unet for fold6: 0.7511243218303015\naverage AJI pure Unet for fold6: 0.42118235059116355\naverage PQ pure Unet for fold6: 0.3831915126338017\n==========\n==========\naverage Dice Unet watershed for fold6: 0.7501893933125886\naverage AJI Unet watershed for fold6: 0.42628307948814714\naverage PQ Unet watershed for fold6: 0.3454884466365528\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 37s 378ms/step - loss: -0.4053 - dice_coef: 0.6140 - val_loss: -0.6632 - val_dice_coef: 0.7770\n\nEpoch 00001: val_dice_coef improved from -inf to 0.77697, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 351ms/step - loss: -0.5430 - dice_coef: 0.7136 - val_loss: -0.6220 - val_dice_coef: 0.7740\n\nEpoch 00002: val_dice_coef did not improve from 0.77697\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 351ms/step - loss: -0.5757 - dice_coef: 0.7348 - val_loss: -0.6737 - val_dice_coef: 0.8009\n\nEpoch 00003: val_dice_coef improved from 0.77697 to 0.80089, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 351ms/step - loss: -0.6103 - dice_coef: 0.7559 - val_loss: -0.6631 - val_dice_coef: 0.7962\n\nEpoch 00004: val_dice_coef did not improve from 0.80089\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 351ms/step - loss: -0.6398 - dice_coef: 0.7752 - val_loss: -0.6609 - val_dice_coef: 0.7935\n\nEpoch 00005: val_dice_coef did not improve from 0.80089\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 350ms/step - loss: -0.6564 - dice_coef: 0.7857 - val_loss: -0.6505 - val_dice_coef: 0.7876\n\nEpoch 00006: val_dice_coef did not improve from 0.80089\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 352ms/step - loss: -0.6802 - dice_coef: 0.8009 - val_loss: -0.6782 - val_dice_coef: 0.8070\n\nEpoch 00007: val_dice_coef improved from 0.80089 to 0.80695, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 348ms/step - loss: -0.6896 - dice_coef: 0.8071 - val_loss: -0.6743 - val_dice_coef: 0.8086\n\nEpoch 00008: val_dice_coef improved from 0.80695 to 0.80864, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 350ms/step - loss: -0.7120 - dice_coef: 0.8213 - val_loss: -0.6727 - val_dice_coef: 0.8093\n\nEpoch 00009: val_dice_coef improved from 0.80864 to 0.80925, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7192 - dice_coef: 0.8260 - val_loss: -0.6815 - val_dice_coef: 0.8179\n\nEpoch 00010: val_dice_coef improved from 0.80925 to 0.81794, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 348ms/step - loss: -0.7301 - dice_coef: 0.8331 - val_loss: -0.6833 - val_dice_coef: 0.8167\n\nEpoch 00011: val_dice_coef did not improve from 0.81794\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7388 - dice_coef: 0.8385 - val_loss: -0.6373 - val_dice_coef: 0.7991\n\nEpoch 00012: val_dice_coef did not improve from 0.81794\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 351ms/step - loss: -0.7434 - dice_coef: 0.8416 - val_loss: -0.6507 - val_dice_coef: 0.8028\n\nEpoch 00013: val_dice_coef did not improve from 0.81794\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 350ms/step - loss: -0.7514 - dice_coef: 0.8463 - val_loss: -0.6841 - val_dice_coef: 0.8220\n\nEpoch 00014: val_dice_coef improved from 0.81794 to 0.82202, saving model to /kaggle/working/models/raw_unet_8.h5\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 351ms/step - loss: -0.7565 - dice_coef: 0.8497 - val_loss: -0.6312 - val_dice_coef: 0.7989\n\nEpoch 00015: val_dice_coef did not improve from 0.82202\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7631 - dice_coef: 0.8538 - val_loss: -0.6348 - val_dice_coef: 0.8016\n\nEpoch 00016: val_dice_coef did not improve from 0.82202\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7736 - dice_coef: 0.8601 - val_loss: -0.6711 - val_dice_coef: 0.8196\n\nEpoch 00017: val_dice_coef did not improve from 0.82202\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7787 - dice_coef: 0.8634 - val_loss: -0.6318 - val_dice_coef: 0.8008\n\nEpoch 00018: val_dice_coef did not improve from 0.82202\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 350ms/step - loss: -0.7835 - dice_coef: 0.8663 - val_loss: -0.6550 - val_dice_coef: 0.8143\n\nEpoch 00019: val_dice_coef did not improve from 0.82202\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 349ms/step - loss: -0.7875 - dice_coef: 0.8688 - val_loss: -0.6401 - val_dice_coef: 0.8050\n\nEpoch 00020: val_dice_coef did not improve from 0.82202\n3/3 [==============================] - 0s 71ms/step\n==========\naverage dice pure Unet for fold7: 0.8328772425896568\naverage AJI pure Unet for fold7: 0.34681862694114834\naverage PQ pure Unet for fold7: 0.3503425633386801\n==========\n==========\naverage Dice Unet watershed for fold7: 0.833014145466732\naverage AJI Unet watershed for fold7: 0.4749050586632298\naverage PQ Unet watershed for fold7: 0.40215728959439984\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 39s 396ms/step - loss: -0.3518 - dice_coef: 0.5632 - val_loss: -0.5035 - val_dice_coef: 0.7162\n\nEpoch 00001: val_dice_coef improved from -inf to 0.71619, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 354ms/step - loss: -0.5156 - dice_coef: 0.6905 - val_loss: -0.6507 - val_dice_coef: 0.8007\n\nEpoch 00002: val_dice_coef improved from 0.71619 to 0.80068, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 358ms/step - loss: -0.5839 - dice_coef: 0.7354 - val_loss: -0.6783 - val_dice_coef: 0.8235\n\nEpoch 00003: val_dice_coef improved from 0.80068 to 0.82349, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 359ms/step - loss: -0.6221 - dice_coef: 0.7601 - val_loss: -0.6853 - val_dice_coef: 0.8316\n\nEpoch 00004: val_dice_coef improved from 0.82349 to 0.83156, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 37s 369ms/step - loss: -0.6444 - dice_coef: 0.7748 - val_loss: -0.6900 - val_dice_coef: 0.8318\n\nEpoch 00005: val_dice_coef improved from 0.83156 to 0.83177, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 34s 345ms/step - loss: -0.6726 - dice_coef: 0.7934 - val_loss: -0.6885 - val_dice_coef: 0.8365\n\nEpoch 00006: val_dice_coef improved from 0.83177 to 0.83652, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 363ms/step - loss: -0.7196 - dice_coef: 0.8236 - val_loss: -0.6916 - val_dice_coef: 0.8486\n\nEpoch 00007: val_dice_coef improved from 0.83652 to 0.84861, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 357ms/step - loss: -0.7418 - dice_coef: 0.8378 - val_loss: -0.7039 - val_dice_coef: 0.8513\n\nEpoch 00008: val_dice_coef improved from 0.84861 to 0.85128, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 34s 348ms/step - loss: -0.7746 - dice_coef: 0.8584 - val_loss: -0.7064 - val_dice_coef: 0.8542\n\nEpoch 00009: val_dice_coef improved from 0.85128 to 0.85420, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 361ms/step - loss: -0.7901 - dice_coef: 0.8680 - val_loss: -0.6912 - val_dice_coef: 0.8513\n\nEpoch 00010: val_dice_coef did not improve from 0.85420\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 365ms/step - loss: -0.8016 - dice_coef: 0.8754 - val_loss: -0.6979 - val_dice_coef: 0.8555\n\nEpoch 00011: val_dice_coef improved from 0.85420 to 0.85546, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 358ms/step - loss: -0.8102 - dice_coef: 0.8805 - val_loss: -0.7019 - val_dice_coef: 0.8560\n\nEpoch 00012: val_dice_coef improved from 0.85546 to 0.85601, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 361ms/step - loss: -0.8202 - dice_coef: 0.8868 - val_loss: -0.7037 - val_dice_coef: 0.8568\n\nEpoch 00013: val_dice_coef improved from 0.85601 to 0.85676, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 354ms/step - loss: -0.8280 - dice_coef: 0.8917 - val_loss: -0.6833 - val_dice_coef: 0.8542\n\nEpoch 00014: val_dice_coef did not improve from 0.85676\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 363ms/step - loss: -0.8339 - dice_coef: 0.8954 - val_loss: -0.6879 - val_dice_coef: 0.8579\n\nEpoch 00015: val_dice_coef improved from 0.85676 to 0.85795, saving model to /kaggle/working/models/raw_unet_9.h5\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 33s 337ms/step - loss: -0.8419 - dice_coef: 0.9003 - val_loss: -0.6753 - val_dice_coef: 0.8567\n\nEpoch 00016: val_dice_coef did not improve from 0.85795\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 33s 337ms/step - loss: -0.8500 - dice_coef: 0.9056 - val_loss: -0.6856 - val_dice_coef: 0.8568\n\nEpoch 00017: val_dice_coef did not improve from 0.85795\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 34s 345ms/step - loss: -0.8542 - dice_coef: 0.9081 - val_loss: -0.6862 - val_dice_coef: 0.8571\n\nEpoch 00018: val_dice_coef did not improve from 0.85795\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 353ms/step - loss: -0.8589 - dice_coef: 0.9111 - val_loss: -0.6726 - val_dice_coef: 0.8512\n\nEpoch 00019: val_dice_coef did not improve from 0.85795\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 36s 362ms/step - loss: -0.8631 - dice_coef: 0.9138 - val_loss: -0.6713 - val_dice_coef: 0.8551\n\nEpoch 00020: val_dice_coef did not improve from 0.85795\n3/3 [==============================] - 0s 69ms/step\n==========\naverage dice pure Unet for fold8: 0.8626795866377902\naverage AJI pure Unet for fold8: 0.34066652058833774\naverage PQ pure Unet for fold8: 0.37325590526131913\n==========\n==========\naverage Dice Unet watershed for fold8: 0.8614069028012258\naverage AJI Unet watershed for fold8: 0.5323041226466071\naverage PQ Unet watershed for fold8: 0.47335424235182516\n==========\nEpoch 1/20\n\nEpoch 00001: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 38s 387ms/step - loss: -0.3514 - dice_coef: 0.5787 - val_loss: -0.4834 - val_dice_coef: 0.6169\n\nEpoch 00001: val_dice_coef improved from -inf to 0.61693, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 2/20\n\nEpoch 00002: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 356ms/step - loss: -0.5502 - dice_coef: 0.7227 - val_loss: -0.6175 - val_dice_coef: 0.7145\n\nEpoch 00002: val_dice_coef improved from 0.61693 to 0.71453, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 3/20\n\nEpoch 00003: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 354ms/step - loss: -0.5991 - dice_coef: 0.7533 - val_loss: -0.6278 - val_dice_coef: 0.7233\n\nEpoch 00003: val_dice_coef improved from 0.71453 to 0.72334, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 4/20\n\nEpoch 00004: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 35s 357ms/step - loss: -0.6291 - dice_coef: 0.7721 - val_loss: -0.6377 - val_dice_coef: 0.7382\n\nEpoch 00004: val_dice_coef improved from 0.72334 to 0.73817, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 5/20\n\nEpoch 00005: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 362ms/step - loss: -0.6553 - dice_coef: 0.7888 - val_loss: -0.6530 - val_dice_coef: 0.7516\n\nEpoch 00005: val_dice_coef improved from 0.73817 to 0.75156, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 6/20\n\nEpoch 00006: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 359ms/step - loss: -0.6929 - dice_coef: 0.8124 - val_loss: -0.6451 - val_dice_coef: 0.7541\n\nEpoch 00006: val_dice_coef improved from 0.75156 to 0.75408, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 7/20\n\nEpoch 00007: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 362ms/step - loss: -0.7309 - dice_coef: 0.8361 - val_loss: -0.6436 - val_dice_coef: 0.7579\n\nEpoch 00007: val_dice_coef improved from 0.75408 to 0.75792, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 8/20\n\nEpoch 00008: LearningRateScheduler setting learning rate to 0.001.\n99/99 [==============================] - 36s 361ms/step - loss: -0.7578 - dice_coef: 0.8525 - val_loss: -0.6604 - val_dice_coef: 0.7749\n\nEpoch 00008: val_dice_coef improved from 0.75792 to 0.77493, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 9/20\n\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 360ms/step - loss: -0.7815 - dice_coef: 0.8668 - val_loss: -0.6697 - val_dice_coef: 0.7823\n\nEpoch 00009: val_dice_coef improved from 0.77493 to 0.78226, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 10/20\n\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 36s 362ms/step - loss: -0.7960 - dice_coef: 0.8755 - val_loss: -0.6462 - val_dice_coef: 0.7700\n\nEpoch 00010: val_dice_coef did not improve from 0.78226\nEpoch 11/20\n\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 357ms/step - loss: -0.8060 - dice_coef: 0.8815 - val_loss: -0.6429 - val_dice_coef: 0.7744\n\nEpoch 00011: val_dice_coef did not improve from 0.78226\nEpoch 12/20\n\nEpoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 356ms/step - loss: -0.8138 - dice_coef: 0.8862 - val_loss: -0.6553 - val_dice_coef: 0.7796\n\nEpoch 00012: val_dice_coef did not improve from 0.78226\nEpoch 13/20\n\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 357ms/step - loss: -0.8211 - dice_coef: 0.8906 - val_loss: -0.6453 - val_dice_coef: 0.7765\n\nEpoch 00013: val_dice_coef did not improve from 0.78226\nEpoch 14/20\n\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 356ms/step - loss: -0.8271 - dice_coef: 0.8942 - val_loss: -0.6292 - val_dice_coef: 0.7700\n\nEpoch 00014: val_dice_coef did not improve from 0.78226\nEpoch 15/20\n\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 358ms/step - loss: -0.8333 - dice_coef: 0.8980 - val_loss: -0.6515 - val_dice_coef: 0.7808\n\nEpoch 00015: val_dice_coef did not improve from 0.78226\nEpoch 16/20\n\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005.\n99/99 [==============================] - 35s 356ms/step - loss: -0.8398 - dice_coef: 0.9020 - val_loss: -0.6289 - val_dice_coef: 0.7762\n\nEpoch 00016: val_dice_coef did not improve from 0.78226\nEpoch 17/20\n\nEpoch 00017: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 356ms/step - loss: -0.8466 - dice_coef: 0.9061 - val_loss: -0.6170 - val_dice_coef: 0.7705\n\nEpoch 00017: val_dice_coef did not improve from 0.78226\nEpoch 18/20\n\nEpoch 00018: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 356ms/step - loss: -0.8511 - dice_coef: 0.9089 - val_loss: -0.6525 - val_dice_coef: 0.7874\n\nEpoch 00018: val_dice_coef improved from 0.78226 to 0.78735, saving model to /kaggle/working/models/raw_unet_10.h5\nEpoch 19/20\n\nEpoch 00019: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 358ms/step - loss: -0.8552 - dice_coef: 0.9114 - val_loss: -0.6424 - val_dice_coef: 0.7851\n\nEpoch 00019: val_dice_coef did not improve from 0.78735\nEpoch 20/20\n\nEpoch 00020: LearningRateScheduler setting learning rate to 0.00025.\n99/99 [==============================] - 35s 355ms/step - loss: -0.8571 - dice_coef: 0.9126 - val_loss: -0.6319 - val_dice_coef: 0.7802\n\nEpoch 00020: val_dice_coef did not improve from 0.78735\n3/3 [==============================] - 0s 72ms/step\n==========\naverage dice pure Unet for fold9: 0.7901589313621723\naverage AJI pure Unet for fold9: 0.4652685554785924\naverage PQ pure Unet for fold9: 0.4235932061614873\n==========\n==========\naverage Dice Unet watershed for fold9: 0.7905937562079286\naverage AJI Unet watershed for fold9: 0.4811805186547506\naverage PQ Unet watershed for fold9: 0.3817684160512935\n==========\n==========\ntotal training time (all 10 folds): 117.0649650533994 minutes\n","output_type":"stream"}]},{"cell_type":"code","source":["import pandas as pd\n","organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n","              'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n","df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_pure_unet, axis = 1),\n","                   'AJI mean': np.mean(AJI_pure_unet, axis = 1),\n","                   'PQ mean': np.mean(PQ_pure_unet, axis = 1)\n","                  })\n","df.to_csv('final_scores_pure_unet.csv', index=False)\n","print('averge overall dice score (pure Unet):',\"{:.2f}\".format(np.mean(dice_pure_unet)*100), '%')\n","print('averge overall AJI score (pure Unet):', \"{:.2f}\".format(np.mean(AJI_pure_unet)*100), '%')\n","print('averge overall PQ score (pure Unet):', \"{:.2f}\".format(np.mean(PQ_pure_unet)*100), '%')\n","df"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:29:38.246871Z","iopub.execute_input":"2024-04-28T17:29:38.247303Z","iopub.status.idle":"2024-04-28T17:29:38.271397Z","shell.execute_reply.started":"2024-04-28T17:29:38.247265Z","shell.execute_reply":"2024-04-28T17:29:38.270466Z"},"trusted":true,"id":"InTgVCUY0y1d","outputId":"33d48c96-6acd-4b28-ffb6-6ab13ce582d0"},"execution_count":null,"outputs":[{"name":"stdout","text":"averge overall dice score (pure Unet): 79.38 %\naverge overall AJI score (pure Unet): 39.03 %\naverge overall PQ score (pure Unet): 37.19 %\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                Oragn  DICE mean  AJI mean   PQ mean\n0  Human_AdrenalGland   0.793777  0.512400  0.473318\n1        Human_Larynx   0.806693  0.466271  0.433086\n2    Human_LymphNodes   0.799531  0.286439  0.310441\n3   Human_Mediastinum   0.842030  0.314033  0.341084\n4      Human_Pancreas   0.713950  0.326310  0.270426\n5        Human_Pleura   0.744981  0.423141  0.360367\n6          Human_Skin   0.751124  0.421182  0.383192\n7        Human_Testes   0.832877  0.346819  0.350343\n8        Human_Thymus   0.862680  0.340667  0.373256\n9  Human_ThyroidGland   0.790159  0.465269  0.423593","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Oragn</th>\n      <th>DICE mean</th>\n      <th>AJI mean</th>\n      <th>PQ mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Human_AdrenalGland</td>\n      <td>0.793777</td>\n      <td>0.512400</td>\n      <td>0.473318</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human_Larynx</td>\n      <td>0.806693</td>\n      <td>0.466271</td>\n      <td>0.433086</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Human_LymphNodes</td>\n      <td>0.799531</td>\n      <td>0.286439</td>\n      <td>0.310441</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Human_Mediastinum</td>\n      <td>0.842030</td>\n      <td>0.314033</td>\n      <td>0.341084</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Human_Pancreas</td>\n      <td>0.713950</td>\n      <td>0.326310</td>\n      <td>0.270426</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Human_Pleura</td>\n      <td>0.744981</td>\n      <td>0.423141</td>\n      <td>0.360367</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Human_Skin</td>\n      <td>0.751124</td>\n      <td>0.421182</td>\n      <td>0.383192</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Human_Testes</td>\n      <td>0.832877</td>\n      <td>0.346819</td>\n      <td>0.350343</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Human_Thymus</td>\n      <td>0.862680</td>\n      <td>0.340667</td>\n      <td>0.373256</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Human_ThyroidGland</td>\n      <td>0.790159</td>\n      <td>0.465269</td>\n      <td>0.423593</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","organ_name = ['Human_AdrenalGland', 'Human_Larynx', 'Human_LymphNodes', 'Human_Mediastinum',\n","              'Human_Pancreas','Human_Pleura', 'Human_Skin', 'Human_Testes' , 'Human_Thymus', 'Human_ThyroidGland']\n","df = pd.DataFrame({'Oragn': organ_name, 'DICE mean': np.mean(dice_unet_watershed, axis = 1),\n","                   'AJI mean': np.mean(AJI_unet_watershed, axis = 1),\n","                   'PQ mean': np.mean(PQ_unet_watershed, axis = 1),\n","                  })\n","df.to_csv('final_scores_unet_watershed.csv', index=False)\n","print('averge overall dice score (Unet + watershed):', \"{:.2f}\".format(np.mean(dice_unet_watershed)*100),'%')\n","print('averge overall AJI score (Unet + watershed):', \"{:.2f}\".format(np.mean(AJI_unet_watershed)*100),'%')\n","print('averge overall PQ score (Unet + watershed):', \"{:.2f}\".format(np.mean(PQ_unet_watershed)*100),'%')\n","\n","df"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:29:42.045269Z","iopub.execute_input":"2024-04-28T17:29:42.045661Z","iopub.status.idle":"2024-04-28T17:29:42.06935Z","shell.execute_reply.started":"2024-04-28T17:29:42.045622Z","shell.execute_reply":"2024-04-28T17:29:42.068237Z"},"trusted":true,"id":"vmDpDcCS0y1e","outputId":"a52c3c18-31cc-47d9-a129-b1316807b27e"},"execution_count":null,"outputs":[{"name":"stdout","text":"averge overall dice score (Unet + watershed): 79.32 %\naverge overall AJI score (Unet + watershed): 48.13 %\naverge overall PQ score (Unet + watershed): 40.94 %\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                Oragn  DICE mean  AJI mean   PQ mean\n0  Human_AdrenalGland   0.793852  0.529815  0.468275\n1        Human_Larynx   0.806295  0.517082  0.436245\n2    Human_LymphNodes   0.798486  0.493673  0.444537\n3   Human_Mediastinum   0.839504  0.521840  0.466726\n4      Human_Pancreas   0.713801  0.395792  0.308833\n5        Human_Pleura   0.745220  0.440173  0.366977\n6          Human_Skin   0.750189  0.426283  0.345488\n7        Human_Testes   0.833014  0.474905  0.402157\n8        Human_Thymus   0.861407  0.532304  0.473354\n9  Human_ThyroidGland   0.790594  0.481181  0.381768","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Oragn</th>\n      <th>DICE mean</th>\n      <th>AJI mean</th>\n      <th>PQ mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Human_AdrenalGland</td>\n      <td>0.793852</td>\n      <td>0.529815</td>\n      <td>0.468275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human_Larynx</td>\n      <td>0.806295</td>\n      <td>0.517082</td>\n      <td>0.436245</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Human_LymphNodes</td>\n      <td>0.798486</td>\n      <td>0.493673</td>\n      <td>0.444537</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Human_Mediastinum</td>\n      <td>0.839504</td>\n      <td>0.521840</td>\n      <td>0.466726</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Human_Pancreas</td>\n      <td>0.713801</td>\n      <td>0.395792</td>\n      <td>0.308833</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Human_Pleura</td>\n      <td>0.745220</td>\n      <td>0.440173</td>\n      <td>0.366977</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Human_Skin</td>\n      <td>0.750189</td>\n      <td>0.426283</td>\n      <td>0.345488</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Human_Testes</td>\n      <td>0.833014</td>\n      <td>0.474905</td>\n      <td>0.402157</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Human_Thymus</td>\n      <td>0.861407</td>\n      <td>0.532304</td>\n      <td>0.473354</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Human_ThyroidGland</td>\n      <td>0.790594</td>\n      <td>0.481181</td>\n      <td>0.381768</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["np.save('dice_pure_unet.npy', dice_pure_unet)\n","np.save('AJI_pure_unet.npy', AJI_pure_unet)\n","np.save('PQ_pure_unet.npy', PQ_pure_unet)\n","\n","np.save('dice_unet_watershed.npy', dice_unet_watershed)\n","np.save('AJI_unet_watershed.npy', AJI_unet_watershed)\n","np.save('PQ_unet_watershed.npy', PQ_unet_watershed)"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:29:45.162478Z","iopub.execute_input":"2024-04-28T17:29:45.162836Z","iopub.status.idle":"2024-04-28T17:29:45.171396Z","shell.execute_reply.started":"2024-04-28T17:29:45.162804Z","shell.execute_reply":"2024-04-28T17:29:45.170422Z"},"trusted":true,"id":"k6nO0let0y1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dice_pure_unet.shape)\n","print(AJI_pure_unet.shape)\n","print(PQ_pure_unet.shape)\n","print(dice_unet_watershed.shape)\n","print(AJI_unet_watershed.shape)\n","print(PQ_unet_watershed.shape)"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:38:46.663946Z","iopub.execute_input":"2024-04-28T17:38:46.664311Z","iopub.status.idle":"2024-04-28T17:38:46.670819Z","shell.execute_reply.started":"2024-04-28T17:38:46.664281Z","shell.execute_reply":"2024-04-28T17:38:46.66984Z"},"trusted":true,"id":"7-0D-P9V0y1e","outputId":"62a016ee-1c9e-4f2e-9ea2-7eeea2aca3c4"},"execution_count":null,"outputs":[{"name":"stdout","text":"(10, 3)\n(10, 3)\n(10, 3)\n(10, 3)\n(10, 3)\n(10, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":["print(\"\\nSample values:\")\n","print(\"dice_pure_unet:\", dice_pure_unet)\n","print(\"AJI_pure_unet:\", AJI_pure_unet)\n","print(\"PQ_pure_unet:\", PQ_pure_unet)\n","print(\"dice_unet_watershed:\", dice_unet_watershed)\n","print(\"AJI_unet_watershed:\", AJI_unet_watershed)\n","print(\"PQ_unet_watershed:\", PQ_unet_watershed)\n","\n","print(\"\\nValue ranges:\")\n","print(\"dice_pure_unet:\", np.min(dice_pure_unet), np.max(dice_pure_unet))\n","print(\"AJI_pure_unet:\", np.min(AJI_pure_unet), np.max(AJI_pure_unet))\n","print(\"PQ_pure_unet:\", np.min(PQ_pure_unet), np.max(PQ_pure_unet))\n","print(\"dice_unet_watershed:\", np.min(dice_unet_watershed), np.max(dice_unet_watershed))\n","print(\"AJI_unet_watershed:\", np.min(AJI_unet_watershed), np.max(AJI_unet_watershed))\n","print(\"PQ_unet_watershed:\", np.min(PQ_unet_watershed), np.max(PQ_unet_watershed))"],"metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:40:29.645867Z","iopub.execute_input":"2024-04-28T17:40:29.646349Z","iopub.status.idle":"2024-04-28T17:40:29.662183Z","shell.execute_reply.started":"2024-04-28T17:40:29.6463Z","shell.execute_reply":"2024-04-28T17:40:29.66102Z"},"trusted":true,"id":"H-8cjhs70y1e","outputId":"c2e9597a-7a93-4ed9-b004-dad3ddf1f026"},"execution_count":null,"outputs":[{"name":"stdout","text":"\nSample values:\ndice_pure_unet: [[0.80461851 0.86908286 0.70762969]\n [0.84577556 0.74803358 0.82627086]\n [0.68431092 0.81330108 0.90098104]\n [0.86852121 0.81976046 0.83780978]\n [0.71348291 0.76020658 0.6681606 ]\n [0.67664321 0.76684989 0.79144911]\n [0.75972113 0.70705918 0.78659266]\n [0.84205846 0.82694352 0.82962974]\n [0.85254502 0.89747537 0.83801838]\n [0.85414277 0.74523757 0.77109645]]\nAJI_pure_unet: [[0.53008141 0.63697611 0.37014216]\n [0.44470205 0.42742875 0.52668208]\n [0.27809621 0.32280899 0.25841171]\n [0.14207717 0.37029867 0.42972401]\n [0.33896387 0.348035   0.29193092]\n [0.30275566 0.4430824  0.5235853 ]\n [0.40663109 0.38691711 0.46999885]\n [0.29103292 0.36654785 0.38287511]\n [0.22055836 0.42316453 0.37827667]\n [0.5234657  0.39459955 0.47774042]]\nPQ_pure_unet: [[0.50969732 0.61252568 0.29773042]\n [0.41773006 0.36210729 0.51942157]\n [0.21142434 0.30708025 0.41281782]\n [0.2708854  0.33314838 0.41921808]\n [0.29569619 0.29037028 0.22521226]\n [0.23334524 0.38159946 0.46615677]\n [0.3079018  0.37987742 0.46179532]\n [0.33887541 0.39255719 0.31959509]\n [0.24715386 0.48164719 0.39096666]\n [0.49972336 0.40223655 0.3688197 ]]\ndice_unet_watershed: [[0.80574087 0.86863965 0.70717546]\n [0.84565101 0.74644574 0.82678924]\n [0.68446814 0.81081454 0.90017611]\n [0.86724249 0.81631136 0.83495819]\n [0.71355234 0.76084456 0.66700749]\n [0.67673069 0.76687699 0.79205104]\n [0.7597109  0.70613254 0.78472474]\n [0.84094499 0.82848739 0.82961006]\n [0.85078126 0.89770139 0.83573806]\n [0.85414277 0.74616068 0.77147782]]\nAJI_unet_watershed: [[0.55026875 0.64117635 0.39800029]\n [0.5046111  0.48126161 0.56537279]\n [0.38678215 0.47309269 0.6211434 ]\n [0.54820755 0.47690052 0.54041065]\n [0.38131193 0.44988608 0.35617665]\n [0.38046444 0.46361657 0.47643686]\n [0.41226516 0.38537863 0.48120545]\n [0.49112705 0.50350385 0.43008427]\n [0.48341612 0.59618715 0.5173091 ]\n [0.53871692 0.44912903 0.4556956 ]]\nPQ_unet_watershed: [[0.49904105 0.58317649 0.32260748]\n [0.41209353 0.38769463 0.50894553]\n [0.30918315 0.39825436 0.6261733 ]\n [0.52141743 0.40498813 0.47377116]\n [0.29842206 0.34279364 0.28528338]\n [0.28043286 0.40748259 0.41301501]\n [0.30028382 0.33731731 0.3988642 ]\n [0.41876087 0.44253983 0.34517116]\n [0.4156986  0.56913512 0.435229  ]\n [0.43276473 0.37473281 0.33780771]]\n\nValue ranges:\ndice_pure_unet: 0.6681605975723622 0.9009810416075202\nAJI_pure_unet: 0.14207717063304917 0.636976106541324\nPQ_pure_unet: 0.21142434328185875 0.6125256788550083\ndice_unet_watershed: 0.667007494045912 0.9001761133443371\nAJI_unet_watershed: 0.356176649291676 0.6411763501198059\nPQ_unet_watershed: 0.28043286025775727 0.6261732988152398\n","output_type":"stream"}]}]}